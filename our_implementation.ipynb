{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä **Re-Implementation of \"Predicting Food Crises Using News Streams\"**\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç **Objective**\n",
    "\n",
    "This notebook aims to **reproduce and analyze** the methodology presented in the paper:\n",
    "\n",
    "üìÑ **Paper:** [Predicting food crises using news streams](https://www.science.org/doi/10.1126/sciadv.abm3449)  \n",
    "üìä **Dataset:** [Harvard Dataverse Repository](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CJDWUW)  \n",
    "üìú **Original Code & Methods:** [GitHub - Regression Modeling (Step 5)](https://github.com/philippzi98/food_insecurity_predictions_nlp/blob/main/Step%205%20-%20Regression%20Modelling/README.md)\n",
    "\n",
    "---\n",
    "\n",
    "#### üõ† **Methodology**\n",
    "\n",
    "This implementation follows the **key steps** outlined in the paper to predict **food insecurity crises** using a combination of:\n",
    "1Ô∏è‚É£ **Traditional Risk Factors** (conflict, climate, food prices, etc.)  \n",
    "2Ô∏è‚É£ **News-Based Indicators** (text feature frequencies from news articles)  \n",
    "3Ô∏è‚É£ **Lagging & Aggregation** (temporal dependencies at district, province, and country levels)  \n",
    "4Ô∏è‚É£ **Machine Learning Models** (Random Forest, OLS, Lasso)\n",
    "\n",
    "---\n",
    "\n",
    "#### üîó **Reference Materials**\n",
    "\n",
    "üìÑ **Supplementary Material:** Available in `supplemental_material_from_paper.pdf`  \n",
    "üìä **Datasets Used:**\n",
    "\n",
    "- `time_series_with_causes_zscore_full.csv` (Main dataset with time-series features)\n",
    "- `famine-country-province-district-years-CS.csv` (Food insecurity classification)\n",
    "- `matching_districts.csv` (Geographical standardization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìöüîß Import Libraries\n",
    "\n",
    "In this notebook, we will use uv to manage our Python environment and packages efficiently. uv is a modern and fast package manager that simplifies virtual environment creation, and dependency installation. We will create a virtual environment, install necessary libraries, and ensure our environment stays consistent across different setups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncoment the below cell to install `uv` if you have not already. You can also install it trhiugh `pip` by running `!pip install uv` but this will be within your current python environment and not globally.\n",
    "\n",
    "# !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "# !uv venv world-bank\n",
    "# !source world-bank/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "from fuzzywuzzy import fuzz\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://drive.google.com/uc?id=1YoQ1hz9RlaLr2xW3KoKCfJPyyO2PErym\"\n",
    "output = \"data.zip\"\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    gdown.download(url, output, quiet=False) \n",
    "    zipfile.ZipFile('data.zip', 'r').extractall()\n",
    "else:\n",
    "    print(\"You already have the data downloaded and extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load and Clean Data\n",
    "\n",
    "**Understanding the Time-Series Dataset & Column Selection**\n",
    "\n",
    "This dataset contains **district-level time-series data** on food insecurity risk factors, including:\n",
    "\n",
    "- **üìÖ Temporal Information:** `year`, `month`, `year_month`\n",
    "- **üìç Geographical Identifiers:** `admin_code`, `admin_name`, `province`, `country`\n",
    "- **üåç Traditional Risk Factors:** Climate (`rain_mean`, `ndvi_mean`), conflict (`acled_count`), food prices (`p_staple_food`)\n",
    "- **üì∞ News-Based Indicators:** Proportions of news articles mentioning crisis-related keywords (`conflict_0`, `famine_0`, etc.)\n",
    "- **üìâ Food Insecurity Label:** `fews_ipc` (Integrated Phase Classification)\n",
    "\n",
    "üî• **Columns We Will Drop & Why**\n",
    "‚úî **Redundant Aggregations:** `_1`, `_2` columns (province & country-level values) since we will recompute aggregations from scratch anyways.  \n",
    "‚úî **Unnamed/Index Columns:** `Unnamed: 0` as it is unnecessary. It is just a duplicate of default index.\n",
    "‚úî **Unnecessary Identifiers:** If `admin_code` and `admin_name`, after matching these to `matching_districts.csv`, we can drop them.\n",
    "\n",
    "---\n",
    "\n",
    "> ‚ö†Ô∏è **NOTE:**  \n",
    "> For a detailed explanation of the dataset and features, refer to the [`explore_time_series.ipynb`](./explore_time_series.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pd.read_csv('./data/time_series_with_causes_zscore_full.csv', nrows=30)\n",
    "admins = pd.read_csv('./data/famine-country-province-district-years-CS.csv')\n",
    "valid_matching = pd.read_csv('./data/matching_districts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(time_series.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_variant_traditional_factors = [ 'p_staple_food']\n",
    "t_variant_traditional_factors = ['ndvi_mean', 'ndvi_anom', 'rain_mean', 'rain_anom', 'et_mean', 'et_anom', \n",
    "                                    'acled_count', 'acled_fatalities', 'p_staple_food']\n",
    "t_invariant_traditional_factors = ['area', 'cropland_pct', 'pop', 'ruggedness_mean', 'pasture_pct']\n",
    "news_factors = [name for name in time_series.columns.values if '_0' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_factors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns count BEFORE dropping: \", len(time_series.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"Unnamed: 0\", \"centx\", \"centy\", 'change_fews', 'fews_ha', 'fews_proj_med', 'fews_proj_med_ha', 'fews_proj_near_ha'] + [col for col in time_series.columns if col.endswith(('_1', '_2', '_3'))]\n",
    "time_series.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_extra_cols = set(time_series.columns.values) - set(t_variant_traditional_factors) - set(t_invariant_traditional_factors) - set(news_factors)\n",
    "potential_extra_cols = [col for col in potential_extra_cols if not col.endswith(('_1', '_2', '_3'))]\n",
    "print(\"Potential extra columns\", potential_extra_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns count after dropping: \", len(time_series.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåç Admin Level Mapping: Standardizing Geographical Identifiers\n",
    "\n",
    "In this section, we will **map and standardize** the `admin_code` and `admin_name` fields to their corresponding **district, province, and country names**. This step is **crucial** for ensuring **consistency** across different datasets and enabling **accurate aggregations** at multiple administrative levels.\n",
    "\n",
    "üõ† **Why is Admin Level Mapping Important?**\n",
    "‚úÖ Different datasets may use **slightly different spellings or formats** for district names.  \n",
    "‚úÖ Some district names might be **missing or misspelled**, requiring standardization.  \n",
    "‚úÖ We need to **match and align** district names across various sources before aggregating at **province and country levels**.  \n",
    "‚úÖ Proper mapping allows us to **merge datasets correctly** without losing information.  \n",
    "\n",
    "üìå **Steps in Admin Mapping**\n",
    "1Ô∏è‚É£ **Load the `matching_districts.csv` file**, which provides the mapping between different district name variations.  \n",
    "2Ô∏è‚É£ **Identify missing or unmatched `admin_name` values** and find their closest matches using fuzzy matching techniques.  \n",
    "3Ô∏è‚É£ **Ensure that each `admin_code` uniquely maps to one `district`, `province`, and `country`.**  \n",
    "4Ô∏è‚É£ **Replace inconsistent names** in the dataset with their standardized versions.  \n",
    "5Ô∏è‚É£ **Aggregate data at the `province` and `country` levels** after ensuring all districts are correctly mapped.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(admins.country.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admins.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_names = time_series['admin_name'].unique()\n",
    "districts = admins['district'].unique()\n",
    "provinces = admins['province'].unique()\n",
    "countries = admins['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(admin_names), len(districts), len(provinces), len(countries))\n",
    "print (len(set(admin_names).difference(districts)))\n",
    "missing_admin_names = set(admin_names).difference(districts)\n",
    "print (len(missing_admin_names.difference(provinces)))\n",
    "missing_admin_names = missing_admin_names.difference(provinces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy String Matching for Missing Names\n",
    "\n",
    "The function uses **fuzzy string matching** to find the best approximate matches for missing administrative names (e.g., districts and provinces). \n",
    "\n",
    "- Finds the **best matching district/province** for each missing name.\n",
    "- Uses **fuzzy string matching** to calculate the similarity between missing names and known names.\n",
    "- Returns a dictionary that maps each missing name to its closest match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching(missing, names):\n",
    "    matching_districts = {}\n",
    "    for m in missing:\n",
    "        max_overlap = 0\n",
    "        nearest_d = None\n",
    "        for d in names:\n",
    "            d = str(d)\n",
    "            dist = fuzz.partial_ratio(m, d)\n",
    "            if dist > max_overlap:\n",
    "                max_overlap = dist\n",
    "                nearest_d = d\n",
    "        matching_districts[m] = nearest_d\n",
    "    return matching_districts\n",
    "\n",
    "\n",
    "matching = find_matching(missing_admin_names, districts)\n",
    "matching_p = find_matching(missing_admin_names, provinces)\n",
    "\n",
    "# manually verify matching and update\n",
    "for k in matching.keys():\n",
    "    print (k, matching[k], matching_p[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Decoding\n",
    "\n",
    "`to_ascii_escaped(s)`: Converts a Unicode string to an ASCII-safe representation using **unicode-escape**.\n",
    "\n",
    "`from_ascii_escaped(escaped)`: Converts the escaped ASCII string back into its original Unicode form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii_escaped(s):\n",
    "    \"\"\"\n",
    "    Convert a Unicode string to an ASCII-safe string using unicode-escape.\n",
    "    This will replace non-ASCII characters with their escape sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8')\n",
    "    # Using 'unicode-escape' encoding produces a bytes object,\n",
    "    # then decode it to get an ASCII string.\n",
    "    return s.encode('unicode-escape').decode('ascii')\n",
    "\n",
    "def from_ascii_escaped(escaped):\n",
    "    \"\"\"\n",
    "    Convert the ASCII-escaped string back to the original Unicode string.\n",
    "    \"\"\"\n",
    "    # Encode the ASCII string to bytes, then decode using 'unicode-escape'\n",
    "    return escaped.encode('ascii').decode('unicode-escape')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Province for a Given District or Province\n",
    "\n",
    "`find_province(x)`, finds the **province** corresponding to a given administrative name. It accounts for:\n",
    "- **Direct Lookups** (Exact match in known district/province lists)\n",
    "- **Fuzzy Matching** (Using ASCII-safe transformation for inconsistent text encoding)\n",
    "- **Validation Against a Predefined Mapping (`valid_matching`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matched globally\n",
    "matched = valid_matching['missing'].unique()\n",
    "\n",
    "def to_ascii_escaped(s):\n",
    "    \"\"\"\n",
    "    Convert a Unicode string to an ASCII-safe string using unicode-escape.\n",
    "    This will replace non-ASCII characters with their escape sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8')\n",
    "    return s.encode('unicode-escape').decode('ascii')\n",
    "\n",
    "def find_province(x):\n",
    "    try:\n",
    "        # Ensure x is a Unicode string.\n",
    "        if isinstance(x, bytes):\n",
    "            x = x.decode('utf-8')\n",
    "        \n",
    "        # Direct lookup in districts or provinces.\n",
    "        if x in districts:\n",
    "            return admins[admins['district'] == x]['province'].values[0]\n",
    "        elif x in provinces:\n",
    "            return x\n",
    "\n",
    "        # Convert x to an ASCII-escaped version.\n",
    "        escaped_x = to_ascii_escaped(x)\n",
    "        \n",
    "        # Check if the escaped version is in matched.\n",
    "        if escaped_x in matched:\n",
    "            v = valid_matching[valid_matching['missing'] == escaped_x]\n",
    "            if v['match'].values[0] == 'district':\n",
    "                x2 = v['district'].values[0]\n",
    "                return admins[admins['district'] == x2]['province'].values[0]\n",
    "            elif v['match'].values[0] == 'province':\n",
    "                return v['province'].values[0]\n",
    "        \n",
    "        # If no conditions are met, raise an exception.\n",
    "        raise Exception(\"No matching province found\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Province not found for: {} ({})\".format(x, e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Admin Names with Accented Characters and Mapping to Provinces\n",
    "\n",
    "Maps `admin_names` to provinces using the `find_province(a)` function.  \n",
    "If a **direct lookup fails**, it tries to handle cases where the **admin name contains accented characters** (`√©`, `√®`, `√¥`) ->  (encoding decoding issues resolved through directly replacing these with 'e' or 'o', leads to finding a valid match). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_to_province = {}\n",
    "for a in admin_names:\n",
    "    try:\n",
    "        admin_to_province[a] = find_province(a)\n",
    "    except Exception as e:\n",
    "        # Print the admin name that caused an error\n",
    "        print(\"Error with:\", a)\n",
    "        # Check if a contains accented characters \"√©\" or \"√®\"\n",
    "        if '√©' in a or '√®' in a or '√¥' in a:\n",
    "            a_modified = a.replace('√©', 'e').replace('√®', 'e').replace('√¥', 'o')\n",
    "            # Check if the modified name is in districts\n",
    "            if a_modified in districts:\n",
    "                # Use the modified name to look up the province from admins\n",
    "                try:\n",
    "                    province = admins[admins['district'] == a_modified]['province'].values[0]\n",
    "                    admin_to_province[a] = province\n",
    "                    print(f\"Replaced '{a}' with '{a_modified}', found province: {province}\")\n",
    "                except Exception as ex:\n",
    "                    print(f\"Modified name '{a_modified}' not found in admins: {ex}\")\n",
    "            else:\n",
    "                print(f\"Modified name '{a_modified}' not in districts.\")\n",
    "        else:\n",
    "            print(f\"No accented e found in '{a}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Administrative Names to Provinces in time_series\n",
    "\n",
    "Maps `admin_name` to their respective **provinces** using a precomputed dictionary - >`admin_to_province` in `time_series`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series['province'] = time_series['admin_name'].apply(\n",
    "    lambda x: admin_to_province[x] if x in admin_to_province else admin_to_province.get(x.replace('√¥', 'o'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series[[\"admin_name\", \"province\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è≥ Time Lagging & Feature Engineering\n",
    "\n",
    "#### üìÖ **Why Use Lagging?**\n",
    "\n",
    "To predict food insecurity **for a given quarter**, we use:\n",
    "\n",
    "- **6 months of historical values** for traditional & news-based features.\n",
    "- **Province & country-level aggregations** to capture broader shocks.\n",
    "- **6 quarters of lagged IPC phase values** to model temporal dependencies.\n",
    "\n",
    "#### ‚ö° **Optimized Lagging Approach**\n",
    "\n",
    "To improve computational efficiency, we:\n",
    "‚úî Use `groupby()` for **fast province & country-level aggregations**.  \n",
    "‚úî Merge lagged data via `merge()` instead of slow `.apply()`.  \n",
    "‚úî Only keep **past data** to ensure no data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_lagged(features, start=3, end=9, diff=1, agg=True, time_series=time_series):\n",
    "    levels = ['', '_province', '_country'] if agg else ['']\n",
    "    \n",
    "    # Work on a copy to avoid modifying the original during processing\n",
    "    working_df = time_series.copy()\n",
    "    \n",
    "    # Precompute a mapping for each feature (with its suffix) for fast lookups.\n",
    "    # For each row, its lookup key will be: admin_code + '_' + year_month.\n",
    "    lookup_maps = {}  # dict mapping f_s -> mapping dict\n",
    "    for suffix in levels:\n",
    "        for f in features:\n",
    "            f_s = f + suffix\n",
    "            # Build a mapping from key to first occurrence of f_s value.\n",
    "            # Key: admin_code + '_' + year_month\n",
    "            keys = working_df['admin_code'].astype(str) + '_' + working_df['year_month'].astype(str)\n",
    "            # If there are duplicates, the first occurrence will be used.\n",
    "            mapping = dict(zip(keys, working_df[f_s]))\n",
    "            lookup_maps[f_s] = mapping\n",
    "\n",
    "    # Prepare list to collect all new columns (as Series)\n",
    "    new_cols = {}\n",
    "    \n",
    "    # Process each feature and lag combination\n",
    "    for suffix in levels:\n",
    "        for f in features:\n",
    "            f_s = f + suffix\n",
    "            mapping = lookup_maps[f_s]\n",
    "            for t in range(start, end, diff):\n",
    "                col_name = f\"{f_s}_{t}\"\n",
    "                if col_name in time_series.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Compute lagged month and lagged year (vectorized)\n",
    "                month = working_df['month']\n",
    "                year = working_df['year']\n",
    "                l_month = ((month - 1 - t) % 12) + 1\n",
    "                l_year = np.where(month - t <= 0, year - 1, year) # If (month - t) is less than or equal to 0 (i.e., you‚Äôve gone into the previous year), then l_year is year - 1; otherwise, it remains year.\n",
    "                \n",
    "                # Build the reference key: admin_code + '_' + \"{l_year}_{l_month}\"\n",
    "                ref_key = working_df['admin_code'].astype(str) + '_' + \\\n",
    "                          l_year.astype(str) + '_' + \\\n",
    "                          l_month.astype(str)\n",
    "                \n",
    "                # Map the reference key to the lagged feature values using our precomputed mapping.\n",
    "                # Where no match is found, use the current value from working_df[f_s].\n",
    "                lagged_values = ref_key.map(mapping)\n",
    "                lagged_values = lagged_values.fillna(working_df[f_s])\n",
    "                \n",
    "                # Store the new column in our dictionary (preserving the original index)\n",
    "                new_cols[col_name] = lagged_values\n",
    "                \n",
    "    # If any new columns were created, add them to the original time_series DataFrame.\n",
    "    if new_cols:\n",
    "        new_cols_df = pd.DataFrame(new_cols, index=working_df.index)\n",
    "        time_series = pd.concat([time_series, new_cols_df], axis=1)\n",
    "        \n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Province & Country-Level Aggregation\n",
    "\n",
    "This function aggregates feature values at the province and country levels to capture regional trends, aiding in food insecurity prediction. The process includes:\n",
    "\n",
    "- **Grouping by year_month and level:** Data is grouped by year_month and the specified level (province or country) to calculate the mean of features, reflecting regional trends over time.\n",
    "\n",
    "- **Applying transformations efficiently:** Instead of merging aggregated data, `transform(\"mean\")` is used to directly assign the computed mean to each row, avoiding unnecessary joins and improving performance.  \n",
    "\n",
    "#### ‚ö° **Efficiency Gains**\n",
    "\n",
    "- **Fast Aggregation**: Uses `groupby()` for efficient aggregation.\n",
    "- **Avoids Costly Joins**: Eliminates the need for `merge()` by using `transform()` instead, reducing computational overhead.  \n",
    "- **Memory Efficiency**: Converts the `level` column to a categorical type to reduce memory usage.\n",
    "\n",
    "This approach ensures faster processing while maintaining the quality of aggregated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_agg_factors(features, level='province'):\n",
    "    global time_series  \n",
    "\n",
    "    # Convert 'level' column to categorical for performance\n",
    "    time_series[level] = time_series[level].astype('category')\n",
    "    \n",
    "    # Compute grouped mean values for the given features\n",
    "    # TODO : Explain these arguments\n",
    "    grouped_df = time_series.groupby(['year_month', level], observed=True, sort=False)[features].transform(\"mean\")\n",
    "\n",
    "    # Rename columns to include level\n",
    "    grouped_df = grouped_df.rename(columns={f: f\"{f}_{level}\" for f in features})\n",
    "\n",
    "    # Use pd.concat() to add all columns at once, avoiding fragmentation\n",
    "    time_series = pd.concat([time_series, grouped_df], axis=1)\n",
    "\n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(news_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(news_factors, level='country')\n",
    "time_series.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(t_variant_traditional_factors, level='province')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(t_variant_traditional_factors, level='country')\n",
    "t = add_agg_factors(t_invariant_traditional_factors, level='province')\n",
    "t = add_agg_factors(t_invariant_traditional_factors, level='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series.to_csv('ours_agg_province_features_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add time lagged features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(t_variant_traditional_factors, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(news_factors, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(['fews_ipc'], end=21, diff=3, agg=False, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(['fews_proj_near'], start=3, end=4, diff=1, agg=False, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diebold_mariano(preds, labels):\n",
    "    sq_error = [(p-l)**2 for p,l in zip(preds, labels)]\n",
    "    mean = np.mean(sq_error)\n",
    "    n = len(preds)\n",
    "    gammas = {}\n",
    "    m = max(n,int(math.ceil(np.cbrt(n))+2))\n",
    "    for k in range(m):\n",
    "        gammas[k] = 0\n",
    "        for i in range(k+1, n):\n",
    "            gammas[k] += (sq_error[i] - mean)*(sq_error[i-k] - mean)\n",
    "        gammas[k] = gammas[k]/n\n",
    "    sum_gamma = gammas[0]\n",
    "    for k in range(1, m):\n",
    "        sum_gamma += 2*gammas[k]\n",
    "    return np.sqrt(sum_gamma/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.to_csv(\"our_results_final_all_30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns that contain a particular substring\n",
    "\n",
    "# list(filter(lambda x: 'co' in x, time_series.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns that begin with a particular substring\n",
    "list(filter(lambda x: x.startswith(\"coun\"), time_series.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and save data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import root_mean_squared_error, precision_recall_curve, auc\n",
    "\n",
    "train_splits = [((2009,7), (2010,4)), ((2009,7), (2011,1)), ((2009,7), (2011,10)), \n",
    "                ((2009,7), (2012,7)), ((2009,7), (2013,7)), ((2009,7), (2014,1)), \n",
    "                ((2009,7), (2015,1)), ((2009,7), (2015,10)), ((2009,7), (2016,10)), \n",
    "                ((2009,7), (2017,2))]\n",
    "\n",
    "dev_splits = [((2010,4), (2010, 7)), ((2011,1), (2011, 7)), ((2011,10), (2012, 7)), \n",
    "              ((2012,7), (2013, 7)), ((2013,4), (2014, 7)), ((2014,1), (2015, 7)), \n",
    "              ((2015,1), (2016, 7)), ((2015,10), (2017, 7)), ((2016,10), (2018, 7)), \n",
    "              ((2017,2), (2019, 2))]\n",
    "\n",
    "test_splits = [((2010,7), (2011, 7)), ((2011,7), (2012, 7)), ((2012,7), (2013, 7)), \n",
    "               ((2013,7), (2014, 7)), ((2014,7), (2015, 7)), ((2015,7), (2016, 7)), \n",
    "               ((2016,7), (2017, 7)), ((2017,7), (2018, 7)), ((2018,7), (2019, 7)), \n",
    "               ((2019,2), (2020, 2))]\n",
    "\n",
    "# just like them we will evaluate three dufferent models, Random Forest, OLS and Lasso. Random Forest is a tree-based model, OLS is a linear regression model and Lasso is a linear regression model with L1 regularization\n",
    "models = {\n",
    "    'RF': RandomForestRegressor(max_features='sqrt', n_estimators=100, min_samples_split=0.5, min_impurity_decrease=0.001, random_state=0),\n",
    "    'OLS': LinearRegression(),\n",
    "    'Lasso': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "def get_agg_lagged_features(factors):\n",
    "    return [f\"{f}_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_province_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_country_{t}\" for f in factors for t in range(3, 9)]\n",
    "\n",
    "features = {\n",
    "    'traditional': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors],\n",
    "    \n",
    "    'news': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(news_factors)],\n",
    "    \n",
    "    'traditional+news': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors + \n",
    "        get_agg_lagged_features(news_factors)]\n",
    "}\n",
    "\n",
    "labels_df = time_series[['fews_ipc', 'year', 'month']]\n",
    "\n",
    "def get_time_split(df, start, end):\n",
    "    return df[\n",
    "        (((df['year'] > start[0])) | ((df['year'] == start[0]) & (df['month'] >= start[1]))) &\n",
    "        (((df['year'] < end[0])) | ((df['year'] == end[0]) & (df['month'] <= end[1])))\n",
    "    ]\n",
    "\n",
    "thresholds = {\n",
    "    'traditional': (2.236, 3.125), 'news': (1.907, 2.712), 'traditional+news': (2.105, 3.314),\n",
    "} # (lowerbound, upperbound)\n",
    "\n",
    "def train_and_evaluate(train, dev, test, f, D):\n",
    "    results = []\n",
    "\n",
    "    X_train = get_time_split(D, train[0], dev[1]).drop(columns=['year', 'month']).fillna(0).to_numpy() # not sure how okay it is to do fillna. When me and Bilal were running this we were getting the error that cannot run the model on NaN values. First we dropped na but this was causing the shape of the X_train to be different from the y_train. So we decided to fillna with 0. - aysha & bilal\n",
    "    y_train = get_time_split(labels_df, train[0], dev[1]).drop(columns=['year', 'month']).to_numpy().ravel()\n",
    "    \n",
    "    X_test = get_time_split(D, test[0], test[1]).drop(columns=['year', 'month']).fillna(0).to_numpy()\n",
    "    y_test = get_time_split(labels_df, test[0], test[1]).drop(columns=['year', 'month']).to_numpy().ravel()\n",
    "    \n",
    "    # convert y_test into binary classification (1 if inside threshold, else 0)\n",
    "    lower, upper = thresholds[f]\n",
    "    y_test_binary = np.where((y_test >= lower) & (y_test <= upper), 1, 0)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        rmse = root_mean_squared_error(y_test, preds)\n",
    "\n",
    "        stderr = np.std(y_test - preds) / np.sqrt(len(y_test))\n",
    "        upper_bound = np.sqrt(rmse**2 + 1.96 * stderr)\n",
    "        lower_bound = np.sqrt(rmse**2 - 1.96 * stderr)\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_test_binary, preds)\n",
    "        aucpr = auc(recall, precision)\n",
    "\n",
    "        results.append({\n",
    "            'method': name, 'split': test, 'features': f, \n",
    "            'rmse': rmse, 'lower_bound': lower_bound, 'upper_bound': upper_bound,\n",
    "            'aucpr': aucpr\n",
    "        })\n",
    "\n",
    "        print(f\"Method: {name}, Split: {test}, Features: {f}, AUCPR: {aucpr:.4f}\")\n",
    "        print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f} [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "        \n",
    "        # completely removed the part where they were doing country-wise evaluation. Do not see point - aysha\n",
    "    \n",
    "    return results\n",
    "\n",
    "# run in parallel on 4 cpu cores/decrease this if you do not want ur system to crash (speaking from experience)\n",
    "all_results = Parallel(n_jobs=4)(\n",
    "    delayed(train_and_evaluate)(train, dev, test, f, D) for train, dev, test in zip(train_splits, dev_splits, test_splits) for f, D in features.items()\n",
    ")\n",
    "\n",
    "fig_3a = pd.DataFrame([res for sublist in all_results for res in sublist])\n",
    "fig_3a.to_csv('fig_3a.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world-bank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
