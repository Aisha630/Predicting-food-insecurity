{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š **Re-Implementation of \"Predicting Food Crises Using News Streams\"**\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” **Objective**\n",
    "\n",
    "This notebook aims to **reproduce and analyze** the methodology presented in the paper:\n",
    "\n",
    "ðŸ“„ **Paper:** [Predicting food crises using news streams](https://www.science.org/doi/10.1126/sciadv.abm3449)  \n",
    "ðŸ“Š **Dataset:** [Harvard Dataverse Repository](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CJDWUW)  \n",
    "ðŸ“œ **Original Code & Methods:** [GitHub - Regression Modeling (Step 5)](https://github.com/philippzi98/food_insecurity_predictions_nlp/blob/main/Step%205%20-%20Regression%20Modelling/README.md)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ›  **Methodology**\n",
    "\n",
    "This implementation follows the **key steps** outlined in the paper to predict **food insecurity crises** using a combination of:\n",
    "1ï¸âƒ£ **Traditional Risk Factors** (conflict, climate, food prices, etc.)  \n",
    "2ï¸âƒ£ **News-Based Indicators** (text feature frequencies from news articles)  \n",
    "3ï¸âƒ£ **Lagging & Aggregation** (temporal dependencies at district, province, and country levels)  \n",
    "4ï¸âƒ£ **Machine Learning Models** (Random Forest, OLS, Lasso)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”— **Reference Materials**\n",
    "\n",
    "ðŸ“„ **Supplementary Material:** Available in `supplemental_material_from_paper.pdf`  \n",
    "ðŸ“Š **Datasets Used:**\n",
    "\n",
    "- `time_series_with_causes_zscore_full.csv` (Main dataset with time-series features)\n",
    "- `famine-country-province-district-years-CS.csv` (Food insecurity classification)\n",
    "- `matching_districts.csv` (Geographical standardization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“šðŸ”§ Import Libraries\n",
    "\n",
    "In this notebook, we will use uv to manage our Python environment and packages efficiently. uv is a modern and fast package manager that simplifies virtual environment creation, and dependency installation. We will create a virtual environment, install necessary libraries, and ensure our environment stays consistent across different setups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncoment the below cell to install `uv` if you have not already. You can also install it trhiugh `pip` by running `!pip install uv` but this will be within your current python environment and not globally.\n",
    "\n",
    "# !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "# !uv venv world-bank\n",
    "# !source world-bank/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "from fuzzywuzzy import fuzz\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have the data downloaded and extracted\n"
     ]
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?id=1YoQ1hz9RlaLr2xW3KoKCfJPyyO2PErym\"\n",
    "output = \"data.zip\"\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    gdown.download(url, output, quiet=False) \n",
    "    zipfile.ZipFile('data.zip', 'r').extractall()\n",
    "else:\n",
    "    print(\"You already have the data downloaded and extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Load and Clean Data\n",
    "\n",
    "**Understanding the Time-Series Dataset & Column Selection**\n",
    "\n",
    "This dataset contains **district-level time-series data** on food insecurity risk factors, including:\n",
    "\n",
    "- **ðŸ“… Temporal Information:** `year`, `month`, `year_month`\n",
    "- **ðŸ“ Geographical Identifiers:** `admin_code`, `admin_name`, `province`, `country`\n",
    "- **ðŸŒ Traditional Risk Factors:** Climate (`rain_mean`, `ndvi_mean`), conflict (`acled_count`), food prices (`p_staple_food`)\n",
    "- **ðŸ“° News-Based Indicators:** Proportions of news articles mentioning crisis-related keywords (`conflict_0`, `famine_0`, etc.)\n",
    "- **ðŸ“‰ Food Insecurity Label:** `fews_ipc` (Integrated Phase Classification)\n",
    "\n",
    "ðŸ”¥ **Columns We Will Drop & Why**\n",
    "âœ” **Redundant Aggregations:** `_1`, `_2` columns (province & country-level values) since we will recompute aggregations from scratch anyways.  \n",
    "âœ” **Unnamed/Index Columns:** `Unnamed: 0` as it is unnecessary. It is just a duplicate of default index.\n",
    "âœ” **Unnecessary Identifiers:** If `admin_code` and `admin_name`, after matching these to `matching_districts.csv`, we can drop them.\n",
    "\n",
    "---\n",
    "\n",
    "> âš ï¸ **NOTE:**  \n",
    "> For a detailed explanation of the dataset and features, refer to the [`explore_time_series.ipynb`](./explore_time_series.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pd.read_csv('./data/time_series_with_causes_zscore_full.csv', nrows=30)\n",
    "admins = pd.read_csv('./data/famine-country-province-district-years-CS.csv')\n",
    "valid_matching = pd.read_csv('./data/matching_districts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(time_series.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "      <th>admin_code</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>centx</th>\n",
       "      <th>centy</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>carbon_2</th>\n",
       "      <th>mayhem_0</th>\n",
       "      <th>mayhem_1</th>\n",
       "      <th>mayhem_2</th>\n",
       "      <th>dehydrated_0</th>\n",
       "      <th>dehydrated_1</th>\n",
       "      <th>dehydrated_2</th>\n",
       "      <th>mismanagement_0</th>\n",
       "      <th>mismanagement_1</th>\n",
       "      <th>mismanagement_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>65.709343</td>\n",
       "      <td>31.043618</td>\n",
       "      <td>2009_07</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.053000</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>-0.171000</td>\n",
       "      <td>-0.833000</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>1.284667</td>\n",
       "      <td>-0.073000</td>\n",
       "      <td>-0.427667</td>\n",
       "      <td>0.668333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>65.709343</td>\n",
       "      <td>31.043618</td>\n",
       "      <td>2009_10</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660812</td>\n",
       "      <td>-0.636580</td>\n",
       "      <td>-0.520247</td>\n",
       "      <td>-0.782913</td>\n",
       "      <td>-0.671587</td>\n",
       "      <td>-0.612254</td>\n",
       "      <td>-0.926921</td>\n",
       "      <td>-0.510467</td>\n",
       "      <td>-0.625133</td>\n",
       "      <td>-0.452467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>65.709343</td>\n",
       "      <td>31.043618</td>\n",
       "      <td>2010_01</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134333</td>\n",
       "      <td>1.447667</td>\n",
       "      <td>-0.844333</td>\n",
       "      <td>0.778667</td>\n",
       "      <td>-0.676000</td>\n",
       "      <td>-0.689667</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.530333</td>\n",
       "      <td>-0.471333</td>\n",
       "      <td>0.955333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>65.709343</td>\n",
       "      <td>31.043618</td>\n",
       "      <td>2010_04</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326927</td>\n",
       "      <td>-0.594877</td>\n",
       "      <td>0.164790</td>\n",
       "      <td>-0.905210</td>\n",
       "      <td>-0.620540</td>\n",
       "      <td>0.165794</td>\n",
       "      <td>0.045794</td>\n",
       "      <td>-1.011600</td>\n",
       "      <td>-0.810600</td>\n",
       "      <td>-0.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>65.709343</td>\n",
       "      <td>31.043618</td>\n",
       "      <td>2010_07</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.085146</td>\n",
       "      <td>-0.709913</td>\n",
       "      <td>-0.867913</td>\n",
       "      <td>-0.770247</td>\n",
       "      <td>-0.787921</td>\n",
       "      <td>-0.974587</td>\n",
       "      <td>-0.946921</td>\n",
       "      <td>-0.611133</td>\n",
       "      <td>-0.709800</td>\n",
       "      <td>-0.622800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index      country  admin_code admin_name      centx  \\\n",
       "0           0     30  Afghanistan         202   Kandahar  65.709343   \n",
       "1           1     33  Afghanistan         202   Kandahar  65.709343   \n",
       "2           2     36  Afghanistan         202   Kandahar  65.709343   \n",
       "3           3     39  Afghanistan         202   Kandahar  65.709343   \n",
       "4           4     42  Afghanistan         202   Kandahar  65.709343   \n",
       "\n",
       "       centy year_month  year  month  ...  carbon_2  mayhem_0  mayhem_1  \\\n",
       "0  31.043618    2009_07  2009      7  ...  1.053000  0.667000 -0.171000   \n",
       "1  31.043618    2009_10  2009     10  ... -0.660812 -0.636580 -0.520247   \n",
       "2  31.043618    2010_01  2010      1  ... -0.134333  1.447667 -0.844333   \n",
       "3  31.043618    2010_04  2010      4  ... -0.326927 -0.594877  0.164790   \n",
       "4  31.043618    2010_07  2010      7  ... -1.085146 -0.709913 -0.867913   \n",
       "\n",
       "   mayhem_2  dehydrated_0  dehydrated_1  dehydrated_2  mismanagement_0  \\\n",
       "0 -0.833000      0.173667      0.168000      1.284667        -0.073000   \n",
       "1 -0.782913     -0.671587     -0.612254     -0.926921        -0.510467   \n",
       "2  0.778667     -0.676000     -0.689667      0.293333         0.530333   \n",
       "3 -0.905210     -0.620540      0.165794      0.045794        -1.011600   \n",
       "4 -0.770247     -0.787921     -0.974587     -0.946921        -0.611133   \n",
       "\n",
       "   mismanagement_1  mismanagement_2  \n",
       "0        -0.427667         0.668333  \n",
       "1        -0.625133        -0.452467  \n",
       "2        -0.471333         0.955333  \n",
       "3        -0.810600        -0.205600  \n",
       "4        -0.709800        -0.622800  \n",
       "\n",
       "[5 rows x 532 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_variant_traditional_factors = [ 'p_staple_food']\n",
    "t_variant_traditional_factors = ['ndvi_mean', 'ndvi_anom', 'rain_mean', 'rain_anom', 'et_mean', 'et_anom', \n",
    "                                    'acled_count', 'acled_fatalities', 'p_staple_food']\n",
    "t_invariant_traditional_factors = ['area', 'cropland_pct', 'pop', 'ruggedness_mean', 'pasture_pct']\n",
    "news_factors = [name for name in time_series.columns.values if '_0' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'land seizures_0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_factors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns count BEFORE dropping:  532\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns count BEFORE dropping: \", len(time_series.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"Unnamed: 0\", \"centx\", \"centy\", 'change_fews', 'fews_ha', 'fews_proj_med', 'fews_proj_med_ha', 'fews_proj_near_ha'] + [col for col in time_series.columns if col.endswith(('_1', '_2', '_3'))]\n",
    "time_series.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential extra columns ['year', 'year_month', 'index', 'admin_code', 'fews_ipc', 'fews_proj_near', 'admin_name', 'country', 'month']\n"
     ]
    }
   ],
   "source": [
    "potential_extra_cols = set(time_series.columns.values) - set(t_variant_traditional_factors) - set(t_invariant_traditional_factors) - set(news_factors)\n",
    "potential_extra_cols = [col for col in potential_extra_cols if not col.endswith(('_1', '_2', '_3'))]\n",
    "print(\"Potential extra columns\", potential_extra_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns count after dropping:  190\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns count after dropping: \", len(time_series.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Admin Level Mapping: Standardizing Geographical Identifiers\n",
    "\n",
    "In this section, we will **map and standardize** the `admin_code` and `admin_name` fields to their corresponding **district, province, and country names**. This step is **crucial** for ensuring **consistency** across different datasets and enabling **accurate aggregations** at multiple administrative levels.\n",
    "\n",
    "ðŸ›  **Why is Admin Level Mapping Important?**\n",
    "âœ… Different datasets may use **slightly different spellings or formats** for district names.  \n",
    "âœ… Some district names might be **missing or misspelled**, requiring standardization.  \n",
    "âœ… We need to **match and align** district names across various sources before aggregating at **province and country levels**.  \n",
    "âœ… Proper mapping allows us to **merge datasets correctly** without losing information.  \n",
    "\n",
    "ðŸ“Œ **Steps in Admin Mapping**\n",
    "1ï¸âƒ£ **Load the `matching_districts.csv` file**, which provides the mapping between different district name variations.  \n",
    "2ï¸âƒ£ **Identify missing or unmatched `admin_name` values** and find their closest matches using fuzzy matching techniques.  \n",
    "3ï¸âƒ£ **Ensure that each `admin_code` uniquely maps to one `district`, `province`, and `country`.**  \n",
    "4ï¸âƒ£ **Replace inconsistent names** in the dataset with their standardized versions.  \n",
    "5ï¸âƒ£ **Aggregate data at the `province` and `country` levels** after ensuring all districts are correctly mapped.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(admins.country.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'country', 'district', 'year', 'month', 'CS',\n",
       "       'province'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admins.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_names = time_series['admin_name'].unique()\n",
    "districts = admins['district'].unique()\n",
    "provinces = admins['province'].unique()\n",
    "countries = admins['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4113 474 39\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (len(admin_names), len(districts), len(provinces), len(countries))\n",
    "print (len(set(admin_names).difference(districts)))\n",
    "missing_admin_names = set(admin_names).difference(districts)\n",
    "print (len(missing_admin_names.difference(provinces)))\n",
    "missing_admin_names = missing_admin_names.difference(provinces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy String Matching for Missing Names\n",
    "\n",
    "The function uses **fuzzy string matching** to find the best approximate matches for missing administrative names (e.g., districts and provinces). \n",
    "\n",
    "- Finds the **best matching district/province** for each missing name.\n",
    "- Uses **fuzzy string matching** to calculate the similarity between missing names and known names.\n",
    "- Returns a dictionary that maps each missing name to its closest match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching(missing, names):\n",
    "    matching_districts = {}\n",
    "    for m in missing:\n",
    "        max_overlap = 0\n",
    "        nearest_d = None\n",
    "        for d in names:\n",
    "            d = str(d)\n",
    "            dist = fuzz.partial_ratio(m, d)\n",
    "            if dist > max_overlap:\n",
    "                max_overlap = dist\n",
    "                nearest_d = d\n",
    "        matching_districts[m] = nearest_d\n",
    "    return matching_districts\n",
    "\n",
    "\n",
    "matching = find_matching(missing_admin_names, districts)\n",
    "matching_p = find_matching(missing_admin_names, provinces)\n",
    "\n",
    "# manually verify matching and update\n",
    "for k in matching.keys():\n",
    "    print (k, matching[k], matching_p[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Decoding\n",
    "\n",
    "`to_ascii_escaped(s)`: Converts a Unicode string to an ASCII-safe representation using **unicode-escape**.\n",
    "\n",
    "`from_ascii_escaped(escaped)`: Converts the escaped ASCII string back into its original Unicode form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii_escaped(s):\n",
    "    \"\"\"\n",
    "    Convert a Unicode string to an ASCII-safe string using unicode-escape.\n",
    "    This will replace non-ASCII characters with their escape sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8')\n",
    "    # Using 'unicode-escape' encoding produces a bytes object,\n",
    "    # then decode it to get an ASCII string.\n",
    "    return s.encode('unicode-escape').decode('ascii')\n",
    "\n",
    "def from_ascii_escaped(escaped):\n",
    "    \"\"\"\n",
    "    Convert the ASCII-escaped string back to the original Unicode string.\n",
    "    \"\"\"\n",
    "    # Encode the ASCII string to bytes, then decode using 'unicode-escape'\n",
    "    return escaped.encode('ascii').decode('unicode-escape')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Province for a Given District or Province\n",
    "\n",
    "`find_province(x)`, finds the **province** corresponding to a given administrative name. It accounts for:\n",
    "- **Direct Lookups** (Exact match in known district/province lists)\n",
    "- **Fuzzy Matching** (Using ASCII-safe transformation for inconsistent text encoding)\n",
    "- **Validation Against a Predefined Mapping (`valid_matching`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matched globally\n",
    "matched = valid_matching['missing'].unique()\n",
    "\n",
    "def to_ascii_escaped(s):\n",
    "    \"\"\"\n",
    "    Convert a Unicode string to an ASCII-safe string using unicode-escape.\n",
    "    This will replace non-ASCII characters with their escape sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8')\n",
    "    return s.encode('unicode-escape').decode('ascii')\n",
    "\n",
    "def find_province(x):\n",
    "    try:\n",
    "        # Ensure x is a Unicode string.\n",
    "        if isinstance(x, bytes):\n",
    "            x = x.decode('utf-8')\n",
    "        \n",
    "        # Direct lookup in districts or provinces.\n",
    "        if x in districts:\n",
    "            return admins[admins['district'] == x]['province'].values[0]\n",
    "        elif x in provinces:\n",
    "            return x\n",
    "\n",
    "        # Convert x to an ASCII-escaped version.\n",
    "        escaped_x = to_ascii_escaped(x)\n",
    "        \n",
    "        # Check if the escaped version is in matched.\n",
    "        if escaped_x in matched:\n",
    "            v = valid_matching[valid_matching['missing'] == escaped_x]\n",
    "            if v['match'].values[0] == 'district':\n",
    "                x2 = v['district'].values[0]\n",
    "                return admins[admins['district'] == x2]['province'].values[0]\n",
    "            elif v['match'].values[0] == 'province':\n",
    "                return v['province'].values[0]\n",
    "        \n",
    "        # If no conditions are met, raise an exception.\n",
    "        raise Exception(\"No matching province found\")\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Province not found for: {} ({})\".format(x, e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Admin Names with Accented Characters and Mapping to Provinces\n",
    "\n",
    "Maps `admin_names` to provinces using the `find_province(a)` function.  \n",
    "If a **direct lookup fails**, it tries to handle cases where the **admin name contains accented characters** (`Ã©`, `Ã¨`, `Ã´`) ->  (encoding decoding issues resolved through directly replacing these with 'e' or 'o', leads to finding a valid match). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_to_province = {}\n",
    "for a in admin_names:\n",
    "    try:\n",
    "        admin_to_province[a] = find_province(a)\n",
    "    except Exception as e:\n",
    "        # Print the admin name that caused an error\n",
    "        print(\"Error with:\", a)\n",
    "        # Check if a contains accented characters \"Ã©\" or \"Ã¨\"\n",
    "        if 'Ã©' in a or 'Ã¨' in a or 'Ã´' in a:\n",
    "            a_modified = a.replace('Ã©', 'e').replace('Ã¨', 'e').replace('Ã´', 'o')\n",
    "            # Check if the modified name is in districts\n",
    "            if a_modified in districts:\n",
    "                # Use the modified name to look up the province from admins\n",
    "                try:\n",
    "                    province = admins[admins['district'] == a_modified]['province'].values[0]\n",
    "                    admin_to_province[a] = province\n",
    "                    print(f\"Replaced '{a}' with '{a_modified}', found province: {province}\")\n",
    "                except Exception as ex:\n",
    "                    print(f\"Modified name '{a_modified}' not found in admins: {ex}\")\n",
    "            else:\n",
    "                print(f\"Modified name '{a_modified}' not in districts.\")\n",
    "        else:\n",
    "            print(f\"No accented e found in '{a}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Administrative Names to Provinces in time_series\n",
    "\n",
    "Maps `admin_name` to their respective **provinces** using a precomputed dictionary - >`admin_to_province` in `time_series`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series['province'] = time_series['admin_name'].apply(\n",
    "    lambda x: admin_to_province[x] if x in admin_to_province else admin_to_province.get(x.replace('Ã´', 'o'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series[[\"admin_name\", \"province\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â³ Time Lagging & Feature Engineering\n",
    "\n",
    "#### ðŸ“… **Why Use Lagging?**\n",
    "\n",
    "To predict food insecurity **for a given quarter**, we use:\n",
    "\n",
    "- **6 months of historical values** for traditional & news-based features.\n",
    "- **Province & country-level aggregations** to capture broader shocks.\n",
    "- **6 quarters of lagged IPC phase values** to model temporal dependencies.\n",
    "\n",
    "#### âš¡ **Optimized Lagging Approach**\n",
    "\n",
    "To improve computational efficiency, we:\n",
    "âœ” Use `groupby()` for **fast province & country-level aggregations**.  \n",
    "âœ” Merge lagged data via `merge()` instead of slow `.apply()`.  \n",
    "âœ” Only keep **past data** to ensure no data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_lagged(features, start=3, end=9, diff=1, agg=True, time_series=time_series):\n",
    "    levels = ['', '_province', '_country'] if agg else ['']\n",
    "    \n",
    "    # Work on a copy to avoid modifying the original during processing\n",
    "    working_df = time_series.copy()\n",
    "    \n",
    "    # Precompute a mapping for each feature (with its suffix) for fast lookups.\n",
    "    # For each row, its lookup key will be: admin_code + '_' + year_month.\n",
    "    lookup_maps = {}  # dict mapping f_s -> mapping dict\n",
    "    for suffix in levels:\n",
    "        for f in features:\n",
    "            f_s = f + suffix\n",
    "            # Build a mapping from key to first occurrence of f_s value.\n",
    "            # Key: admin_code + '_' + year_month\n",
    "            keys = working_df['admin_code'].astype(str) + '_' + working_df['year_month'].astype(str)\n",
    "            # If there are duplicates, the first occurrence will be used.\n",
    "            mapping = dict(zip(keys, working_df[f_s]))\n",
    "            lookup_maps[f_s] = mapping\n",
    "\n",
    "    # Prepare list to collect all new columns (as Series)\n",
    "    new_cols = {}\n",
    "    \n",
    "    # Process each feature and lag combination\n",
    "    for suffix in levels:\n",
    "        for f in features:\n",
    "            f_s = f + suffix\n",
    "            mapping = lookup_maps[f_s]\n",
    "            for t in range(start, end, diff):\n",
    "                col_name = f\"{f_s}_{t}\"\n",
    "                if col_name in time_series.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Compute lagged month and lagged year (vectorized)\n",
    "                month = working_df['month']\n",
    "                year = working_df['year']\n",
    "                l_month = ((month - 1 - t) % 12) + 1\n",
    "                l_year = np.where(month - t <= 0, year - 1, year) # If (month - t) is less than or equal to 0 (i.e., youâ€™ve gone into the previous year), then l_year is year - 1; otherwise, it remains year.\n",
    "                \n",
    "                # Build the reference key: admin_code + '_' + \"{l_year}_{l_month}\"\n",
    "                ref_key = working_df['admin_code'].astype(str) + '_' + \\\n",
    "                          l_year.astype(str) + '_' + \\\n",
    "                          l_month.astype(str)\n",
    "                \n",
    "                # Map the reference key to the lagged feature values using our precomputed mapping.\n",
    "                # Where no match is found, use the current value from working_df[f_s].\n",
    "                lagged_values = ref_key.map(mapping)\n",
    "                lagged_values = lagged_values.fillna(working_df[f_s])\n",
    "                \n",
    "                # Store the new column in our dictionary (preserving the original index)\n",
    "                new_cols[col_name] = lagged_values\n",
    "                \n",
    "    # If any new columns were created, add them to the original time_series DataFrame.\n",
    "    if new_cols:\n",
    "        new_cols_df = pd.DataFrame(new_cols, index=working_df.index)\n",
    "        time_series = pd.concat([time_series, new_cols_df], axis=1)\n",
    "        \n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Province & Country-Level Aggregation\n",
    "\n",
    "This function aggregates feature values at the province and country levels to capture regional trends, aiding in food insecurity prediction. The process includes:\n",
    "\n",
    "- **Grouping by year_month and level:** Data is grouped by year_month and the specified level (province or country) to calculate the mean of features, reflecting regional trends over time.\n",
    "\n",
    "- **Applying transformations efficiently:** Instead of merging aggregated data, `transform(\"mean\")` is used to directly assign the computed mean to each row, avoiding unnecessary joins and improving performance.  \n",
    "\n",
    "#### âš¡ **Efficiency Gains**\n",
    "\n",
    "- **Fast Aggregation**: Uses `groupby()` for efficient aggregation.\n",
    "- **Avoids Costly Joins**: Eliminates the need for `merge()` by using `transform()` instead, reducing computational overhead.  \n",
    "- **Memory Efficiency**: Converts the `level` column to a categorical type to reduce memory usage.\n",
    "\n",
    "This approach ensures faster processing while maintaining the quality of aggregated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_agg_factors(features, level='province'):\n",
    "    global time_series  \n",
    "\n",
    "    # Convert 'level' column to categorical for performance\n",
    "    time_series[level] = time_series[level].astype('category')\n",
    "    \n",
    "    # Compute grouped mean values for the given features\n",
    "    # TODO : Explain these arguments\n",
    "    grouped_df = time_series.groupby(['year_month', level], observed=True, sort=False)[features].transform(\"mean\")\n",
    "\n",
    "    # Rename columns to include level\n",
    "    grouped_df = grouped_df.rename(columns={f: f\"{f}_{level}\" for f in features})\n",
    "\n",
    "    # Use pd.concat() to add all columns at once, avoiding fragmentation\n",
    "    time_series = pd.concat([time_series, grouped_df], axis=1)\n",
    "\n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(news_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "      <th>admin_code</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>fews_ipc</th>\n",
       "      <th>fews_proj_near</th>\n",
       "      <th>ndvi_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>gastrointestinal_0_country</th>\n",
       "      <th>terrorist_0_country</th>\n",
       "      <th>warlord_0_country</th>\n",
       "      <th>d'etat_0_country</th>\n",
       "      <th>overthrow_0_country</th>\n",
       "      <th>convoys_0_country</th>\n",
       "      <th>carbon_0_country</th>\n",
       "      <th>mayhem_0_country</th>\n",
       "      <th>dehydrated_0_country</th>\n",
       "      <th>mismanagement_0_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2009_07</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192000</td>\n",
       "      <td>-0.284333</td>\n",
       "      <td>-0.668667</td>\n",
       "      <td>0.647333</td>\n",
       "      <td>-0.891333</td>\n",
       "      <td>0.112667</td>\n",
       "      <td>1.265333</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>-0.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2009_10</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.545727</td>\n",
       "      <td>-1.037016</td>\n",
       "      <td>-0.811291</td>\n",
       "      <td>-0.850261</td>\n",
       "      <td>-0.948892</td>\n",
       "      <td>-0.728972</td>\n",
       "      <td>-0.765146</td>\n",
       "      <td>-0.636580</td>\n",
       "      <td>-0.671587</td>\n",
       "      <td>-0.510467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_01</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.506333</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>1.595667</td>\n",
       "      <td>0.571667</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>-0.868333</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>1.447667</td>\n",
       "      <td>-0.676000</td>\n",
       "      <td>0.530333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_04</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.793970</td>\n",
       "      <td>-0.722159</td>\n",
       "      <td>-0.130521</td>\n",
       "      <td>0.047630</td>\n",
       "      <td>0.362613</td>\n",
       "      <td>0.480986</td>\n",
       "      <td>0.026073</td>\n",
       "      <td>-0.594877</td>\n",
       "      <td>-0.620540</td>\n",
       "      <td>-1.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_07</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509394</td>\n",
       "      <td>-0.694350</td>\n",
       "      <td>-1.215958</td>\n",
       "      <td>-0.865261</td>\n",
       "      <td>-1.119225</td>\n",
       "      <td>-1.060638</td>\n",
       "      <td>-0.673479</td>\n",
       "      <td>-0.709913</td>\n",
       "      <td>-0.787921</td>\n",
       "      <td>-0.611133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_10</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.691000</td>\n",
       "      <td>1.168667</td>\n",
       "      <td>-0.279333</td>\n",
       "      <td>-0.296667</td>\n",
       "      <td>1.651333</td>\n",
       "      <td>-0.356000</td>\n",
       "      <td>1.156667</td>\n",
       "      <td>1.202667</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2011_01</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916000</td>\n",
       "      <td>0.334333</td>\n",
       "      <td>-0.847000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.744333</td>\n",
       "      <td>1.328667</td>\n",
       "      <td>-0.705000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.358000</td>\n",
       "      <td>0.357667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2011_04</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.131462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098364</td>\n",
       "      <td>-0.792159</td>\n",
       "      <td>-0.083854</td>\n",
       "      <td>-0.229370</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.181986</td>\n",
       "      <td>-0.413594</td>\n",
       "      <td>0.278123</td>\n",
       "      <td>-0.235540</td>\n",
       "      <td>-0.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2011_07</td>\n",
       "      <td>2011</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820667</td>\n",
       "      <td>-0.696000</td>\n",
       "      <td>-0.588667</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>1.049667</td>\n",
       "      <td>-0.590000</td>\n",
       "      <td>-0.759667</td>\n",
       "      <td>-0.057000</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>1.217667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2011_10</td>\n",
       "      <td>2011</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.103268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>-0.851667</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>1.312000</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>1.056333</td>\n",
       "      <td>1.055333</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>-0.213000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      country  admin_code admin_name year_month  year  month  \\\n",
       "0     30  Afghanistan         202   Kandahar    2009_07  2009      7   \n",
       "1     33  Afghanistan         202   Kandahar    2009_10  2009     10   \n",
       "2     36  Afghanistan         202   Kandahar    2010_01  2010      1   \n",
       "3     39  Afghanistan         202   Kandahar    2010_04  2010      4   \n",
       "4     42  Afghanistan         202   Kandahar    2010_07  2010      7   \n",
       "5     45  Afghanistan         202   Kandahar    2010_10  2010     10   \n",
       "6     48  Afghanistan         202   Kandahar    2011_01  2011      1   \n",
       "7     51  Afghanistan         202   Kandahar    2011_04  2011      4   \n",
       "8     54  Afghanistan         202   Kandahar    2011_07  2011      7   \n",
       "9     57  Afghanistan         202   Kandahar    2011_10  2011     10   \n",
       "\n",
       "   fews_ipc  fews_proj_near  ndvi_mean  ...  gastrointestinal_0_country  \\\n",
       "0       1.0             NaN   0.106035  ...                   -0.192000   \n",
       "1       1.0             NaN   0.103009  ...                   -0.545727   \n",
       "2       2.0             NaN   0.109600  ...                    1.506333   \n",
       "3       2.0             NaN   0.111599  ...                   -0.793970   \n",
       "4       1.0             NaN   0.096943  ...                   -0.509394   \n",
       "5       2.0             NaN   0.095377  ...                   -0.691000   \n",
       "6       2.0             NaN   0.092620  ...                   -0.916000   \n",
       "7       2.0             2.0   0.131462  ...                    0.098364   \n",
       "8       1.0             1.0   0.106885  ...                   -0.820667   \n",
       "9       1.0             1.0   0.103268  ...                    0.339000   \n",
       "\n",
       "   terrorist_0_country  warlord_0_country  d'etat_0_country  \\\n",
       "0            -0.284333          -0.668667          0.647333   \n",
       "1            -1.037016          -0.811291         -0.850261   \n",
       "2             0.455000           1.595667          0.571667   \n",
       "3            -0.722159          -0.130521          0.047630   \n",
       "4            -0.694350          -1.215958         -0.865261   \n",
       "5             1.168667          -0.279333         -0.296667   \n",
       "6             0.334333          -0.847000          1.270000   \n",
       "7            -0.792159          -0.083854         -0.229370   \n",
       "8            -0.696000          -0.588667          0.714000   \n",
       "9            -0.851667           0.421000          1.312000   \n",
       "\n",
       "   overthrow_0_country  convoys_0_country  carbon_0_country  mayhem_0_country  \\\n",
       "0            -0.891333           0.112667          1.265333          0.667000   \n",
       "1            -0.948892          -0.728972         -0.765146         -0.636580   \n",
       "2             0.279000          -0.868333          0.058333          1.447667   \n",
       "3             0.362613           0.480986          0.026073         -0.594877   \n",
       "4            -1.119225          -1.060638         -0.673479         -0.709913   \n",
       "5             1.651333          -0.356000          1.156667          1.202667   \n",
       "6             0.744333           1.328667         -0.705000          0.738000   \n",
       "7             0.050279           0.181986         -0.413594          0.278123   \n",
       "8             1.049667          -0.590000         -0.759667         -0.057000   \n",
       "9             0.010667           1.056333          1.055333          0.844000   \n",
       "\n",
       "   dehydrated_0_country  mismanagement_0_country  \n",
       "0              0.173667                -0.073000  \n",
       "1             -0.671587                -0.510467  \n",
       "2             -0.676000                 0.530333  \n",
       "3             -0.620540                -1.011600  \n",
       "4             -0.787921                -0.611133  \n",
       "5              0.446667                 0.696667  \n",
       "6              0.358000                 0.357667  \n",
       "7             -0.235540                -0.817600  \n",
       "8              0.993333                 1.217667  \n",
       "9              0.652000                -0.213000  \n",
       "\n",
       "[10 rows x 525 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = add_agg_factors(news_factors, level='country')\n",
    "time_series.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(t_variant_traditional_factors, level='province')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = add_agg_factors(t_variant_traditional_factors, level='country')\n",
    "t = add_agg_factors(t_invariant_traditional_factors, level='province')\n",
    "t = add_agg_factors(t_invariant_traditional_factors, level='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series.to_csv('ours_agg_province_features_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add time lagged features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(t_variant_traditional_factors, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(news_factors, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(['fews_ipc'], end=21, diff=3, agg=False, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = add_time_lagged(['fews_proj_near'], start=3, end=4, diff=1, agg=False, time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diebold_mariano(preds, labels):\n",
    "    sq_error = [(p-l)**2 for p,l in zip(preds, labels)]\n",
    "    mean = np.mean(sq_error)\n",
    "    n = len(preds)\n",
    "    gammas = {}\n",
    "    m = max(n,int(math.ceil(np.cbrt(n))+2))\n",
    "    for k in range(m):\n",
    "        gammas[k] = 0\n",
    "        for i in range(k+1, n):\n",
    "            gammas[k] += (sq_error[i] - mean)*(sq_error[i-k] - mean)\n",
    "        gammas[k] = gammas[k]/n\n",
    "    sum_gamma = gammas[0]\n",
    "    for k in range(1, m):\n",
    "        sum_gamma += 2*gammas[k]\n",
    "    return np.sqrt(sum_gamma/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3728,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.to_csv(\"our_results_final_all_30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country</th>\n",
       "      <th>admin_code</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>fews_ipc</th>\n",
       "      <th>fews_proj_near</th>\n",
       "      <th>ndvi_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mismanagement_0_country_6</th>\n",
       "      <th>mismanagement_0_country_7</th>\n",
       "      <th>mismanagement_0_country_8</th>\n",
       "      <th>fews_ipc_3</th>\n",
       "      <th>fews_ipc_6</th>\n",
       "      <th>fews_ipc_9</th>\n",
       "      <th>fews_ipc_12</th>\n",
       "      <th>fews_ipc_15</th>\n",
       "      <th>fews_ipc_18</th>\n",
       "      <th>fews_proj_near_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2009_07</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073000</td>\n",
       "      <td>-0.073000</td>\n",
       "      <td>-0.073000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2009_10</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.510467</td>\n",
       "      <td>-0.510467</td>\n",
       "      <td>-0.510467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_01</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530333</td>\n",
       "      <td>0.530333</td>\n",
       "      <td>0.530333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_04</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.510467</td>\n",
       "      <td>-1.011600</td>\n",
       "      <td>-1.011600</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>202</td>\n",
       "      <td>Kandahar</td>\n",
       "      <td>2010_07</td>\n",
       "      <td>2010</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611133</td>\n",
       "      <td>-0.611133</td>\n",
       "      <td>-0.611133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3728 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      country  admin_code admin_name year_month  year  month  \\\n",
       "0     30  Afghanistan         202   Kandahar    2009_07  2009      7   \n",
       "1     33  Afghanistan         202   Kandahar    2009_10  2009     10   \n",
       "2     36  Afghanistan         202   Kandahar    2010_01  2010      1   \n",
       "3     39  Afghanistan         202   Kandahar    2010_04  2010      4   \n",
       "4     42  Afghanistan         202   Kandahar    2010_07  2010      7   \n",
       "\n",
       "   fews_ipc  fews_proj_near  ndvi_mean  ...  mismanagement_0_country_6  \\\n",
       "0       1.0             NaN   0.106035  ...                  -0.073000   \n",
       "1       1.0             NaN   0.103009  ...                  -0.510467   \n",
       "2       2.0             NaN   0.109600  ...                   0.530333   \n",
       "3       2.0             NaN   0.111599  ...                  -0.510467   \n",
       "4       1.0             NaN   0.096943  ...                  -0.611133   \n",
       "\n",
       "   mismanagement_0_country_7  mismanagement_0_country_8  fews_ipc_3  \\\n",
       "0                  -0.073000                  -0.073000         1.0   \n",
       "1                  -0.510467                  -0.510467         1.0   \n",
       "2                   0.530333                   0.530333         1.0   \n",
       "3                  -1.011600                  -1.011600         2.0   \n",
       "4                  -0.611133                  -0.611133         1.0   \n",
       "\n",
       "   fews_ipc_6  fews_ipc_9  fews_ipc_12  fews_ipc_15  fews_ipc_18  \\\n",
       "0         1.0         1.0          1.0          1.0          1.0   \n",
       "1         1.0         1.0          1.0          1.0          1.0   \n",
       "2         2.0         2.0          2.0          1.0          2.0   \n",
       "3         1.0         2.0          2.0          2.0          1.0   \n",
       "4         1.0         1.0          1.0          1.0          1.0   \n",
       "\n",
       "   fews_proj_near_3  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "\n",
       "[5 rows x 3728 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "# raise Exception(\"Stop here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns that contain a particular substring\n",
    "\n",
    "# list(filter(lambda x: 'co' in x, time_series.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns that begin with a particular substring\n",
    "# list(filter(lambda x: x.startswith(\"coun\"), time_series.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series[[\"fews_proj_near_3\", \"fews_proj_near\", \"year\"]].to_csv(\"fews_proj_near_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and save data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import root_mean_squared_error, precision_recall_curve, auc\n",
    "\n",
    "test_splits = [\n",
    "    ((2010,7), (2011, 7)), \n",
    "    ((2011,7), (2012, 7)),\n",
    "    ((2012,7), (2013, 7)), \n",
    "    ((2013,7), (2014, 7)), \n",
    "    ((2014,7), (2015, 7)), \n",
    "    ((2015,7), (2016, 7)), \n",
    "    ((2016,7), (2017, 7)), \n",
    "    ((2017,7), (2018, 7)),\n",
    "    ((2018,7), (2019, 7)), \n",
    "    ((2019,2), (2020, 2)),\n",
    "]\n",
    "\n",
    "train_splits = [\n",
    "    ((2009,7), (2010,4)),\n",
    "    ((2009,7), (2011,1)),\n",
    "    ((2009,7), (2011,10)),\n",
    "    ((2009,7), (2012,7)),\n",
    "    ((2009,7), (2013,7)),\n",
    "    ((2009,7), (2014,1)),\n",
    "    ((2009,7), (2015,1)),\n",
    "    ((2009,7), (2015,10)),\n",
    "    ((2009,7), (2016,10)),\n",
    "    ((2009,7), (2017,2))]\n",
    "\n",
    "dev_splits = [\n",
    "    ((2010,4),  (2010, 7)),\n",
    "    ((2011,1),  (2011, 7)),\n",
    "    ((2011,10), (2012, 7)),\n",
    "    ((2012,7),  (2013, 7)),\n",
    "    ((2013,4),  (2014, 7)),\n",
    "    ((2014,1),  (2015, 7)),\n",
    "    ((2015,1),  (2016, 7)),\n",
    "    ((2015,10), (2017, 7)),\n",
    "    ((2016,10), (2018, 7)),\n",
    "    ((2017,2),  (2019, 2)),\n",
    "]\n",
    "train_splits = train_splits + dev_splits\n",
    "\n",
    "# just like them we will evaluate three dufferent models, Random Forest, OLS and Lasso. Random Forest is a tree-based model, OLS is a linear regression model and Lasso is a linear regression model with L1 regularization\n",
    "models = {\n",
    "    # 'RF': RandomForestRegressor(max_features='sqrt', n_estimators=100, min_samples_split=0.5, min_impurity_decrease=0.001, random_state=0)\n",
    "    'RF': RandomForestRegressor(\n",
    "        max_features='sqrt',  # Keep this\n",
    "        n_estimators=500,  # Increase trees to reduce variance\n",
    "        min_samples_split=5,  # ðŸš¨ Fix this, should be an integer\n",
    "        min_samples_leaf=2,  # Helps prevent overfitting\n",
    "        max_depth=None,  # Allow full tree growth\n",
    "        bootstrap=True,  # Default setting, makes it more robust\n",
    "        random_state=0\n",
    "    )\n",
    "    # 'OLS': LinearRegression(),\n",
    "    # 'Lasso': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "def get_agg_lagged_features(factors):\n",
    "    return [f\"{f}_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_province_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_country_{t}\" for f in factors for t in range(3, 9)]\n",
    "\n",
    "features = {\n",
    "    'traditional': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors],\n",
    "    \n",
    "    # 'news': time_series[['year', 'month'] + \n",
    "    #     [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "    #     get_agg_lagged_features(news_factors)],\n",
    "    \n",
    "    # 'traditional+news': time_series[['year', 'month'] + \n",
    "    #     [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "    #     get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "    #     t_invariant_traditional_factors + \n",
    "    #     get_agg_lagged_features(news_factors)]\n",
    "    \n",
    "    # 'expert': time_series[['fews_proj_near_3' ] + ['year', 'month']],\n",
    "    \n",
    "    # 'expert+traditional': time_series[ ['year', 'month']+ \n",
    "    #     ['fews_proj_near_3'] +\n",
    "    #     ['{}_{}'.format('fews_ipc', t) for t in range(3,21,3)] + \n",
    "    #     get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "    #     t_invariant_traditional_factors\n",
    "    # ],\n",
    "    # 'expert+news': time_series[ ['year', 'month'] +\n",
    "    #     ['fews_proj_near_3'] +\n",
    "    #     ['{}_{}'.format('fews_ipc', t) for t in range(3,21,3)] +\n",
    "    #     get_agg_lagged_features(news_factors)\n",
    "    # ],\n",
    "    # 'expert+traditional+news': time_series[ ['year', 'month'] +\n",
    "    #     ['fews_proj_near_3'] +\n",
    "    #     ['{}_{}'.format('fews_ipc', t) for t in range(3,21,3)] +\n",
    "    #     get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "    #     t_invariant_traditional_factors +\n",
    "    #     get_agg_lagged_features(news_factors)\n",
    "    # ]\n",
    "}\n",
    "\n",
    "labels_df = time_series[['fews_ipc', 'year', 'month']]\n",
    "\n",
    "def get_time_split(df, start, end):\n",
    "    return df[\n",
    "        (((df['year'] > start[0])) | ((df['year'] == start[0]) & (df['month'] >= start[1]))) &\n",
    "        (((df['year'] < end[0])) | ((df['year'] == end[0]) & (df['month'] <= end[1])))\n",
    "    ]\n",
    "\n",
    "thresholds = {'traditional': (2.236, 3.125), \n",
    "              'news': (1.907, 2.712), \n",
    "              'traditional+news': (2.105, 3.314),\n",
    "            #   'expert': (2, 3),\n",
    "            #   'expert+news': (1.912, 2.813),\n",
    "            #   'expert+traditional': (2.241, 3.132),\n",
    "            #   'expert+traditional+news': (2.172, 3.321)\n",
    "             }\n",
    "\n",
    "def train_and_evaluate(train, dev, test, f, D):\n",
    "    results = []\n",
    "    \n",
    "    D.to_csv(f\"D_features_{f}.csv\")\n",
    "    print(\"The feature is: \", f)\n",
    "    print(\"train split:\", train)\n",
    "    print(\"Shape of \")\n",
    "    \n",
    "\n",
    "    # X_train = get_time_split(D, train[0], train[1]).drop(columns=['year', 'month']).to_numpy()# not sure how okay it is to do fillna. When me and Bilal were running this we were getting the error that cannot run the model on NaN values. First we dropped na but this was causing the shape of the X_train to be different from the y_train. So we decided to fillna with 0. - aysha & bilal\n",
    "    \n",
    "    # y_train = get_time_split(labels_df, train[0], train[1]).drop(columns=['year', 'month']).to_numpy().ravel()\n",
    "    # # print(\"The shape of X_train before removing nans is: \", X_train.shape)\n",
    "    # # shape_before = X_train.shape\n",
    "    # nan_mask = np.isnan(X_train).any(axis=1)\n",
    "    # X_train = X_train[~nan_mask]\n",
    "    # y_train = y_train[~nan_mask]\n",
    "    \n",
    "    \n",
    "    # X_test = get_time_split(D, test[0], test[1]).drop(columns=['year', 'month']).to_numpy()\n",
    "    # y_test = get_time_split(labels_df, test[0], test[1]).drop(columns=['year', 'month']).to_numpy().ravel()\n",
    "    # nan_mask_test = np.isnan(X_test).any(axis=1)\n",
    "    # X_test = X_test[~nan_mask_test]\n",
    "    # y_test = y_test[~nan_mask_test]\n",
    "    \n",
    "    # # X_train.to_csv(f\"X_train_{f}.csv\")\n",
    "    # # print(\"The shape of X_train after removing nans is: \", X_train.shape)\n",
    "    # # print(\"The number of nans removed: \", shape_before[0] - X_train.shape[0])\n",
    "    # # print(\"fraction of nans\", (shape_before[0]-X_train.shape[0])/shape_before[0])\n",
    "    \n",
    "    # # print(f\"Train is {X_train}\")\n",
    "    # # print(f\"Y train is {y_train}\")\n",
    "    # # print(f\"Test is {X_test}\")\n",
    "    # # print(f\"Y test is {y_test}\")\n",
    "    \n",
    "    # # convert y_test into binary classification (1 if inside threshold, else 0)\n",
    "    # lower, upper = thresholds[f]\n",
    "    # y_test_binary = np.where((y_test >= lower) & (y_test <= upper), 1, 0)\n",
    "\n",
    "    # for name, model in models.items():\n",
    "    #     model.fit(X_train, y_train)\n",
    "    #     preds = model.predict(X_test)\n",
    "\n",
    "    #     rmse = root_mean_squared_error(y_test, preds)\n",
    "\n",
    "    #     # stderr = np.std(y_test - preds) / np.sqrt(len(y_test))\n",
    "    #     # upper_bound = np.sqrt(rmse**2 + 1.96 * stderr)\n",
    "    #     # lower_bound = np.sqrt(rmse**2 - 1.96 * stderr)\n",
    "\n",
    "    #     # precision, recall, _ = precision_recall_curve(y_test_binary, preds)\n",
    "    #     # aucpr = auc(recall, precision)\n",
    "\n",
    "    #     results.append({\n",
    "    #         'method': name, 'split': test, 'features': f, \n",
    "    #         'rmse': rmse,\n",
    "    #         # 'rmse': rmse, 'lower_bound': lower_bound, 'upper_bound': upper_bound,\n",
    "    #         # 'aucpr': aucpr\n",
    "    #     })\n",
    "\n",
    "    #     # print(f\"Method: {name}, Split: {test}, Features: {f}, AUCPR: {aucpr:.4f}\")\n",
    "    #     # print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f} [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "    #     print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "    # #     # completely removed the part where they were doing country-wise evaluation. Do not see point - aysha\n",
    "    \n",
    "    return results\n",
    "\n",
    "# run in parallel on 4 cpu cores/decrease this if you do not want ur system to crash (speaking from experience)\n",
    "all_results = Parallel(n_jobs=4)(\n",
    "    delayed(train_and_evaluate)(train, dev, test, f, D) for train, dev, test in zip(train_splits, dev_splits, test_splits) for f, D in features.items()\n",
    ")\n",
    "\n",
    "\n",
    "# all_results = []\n",
    "# for train, dev, test in zip(train_splits, dev_splits, test_splits):\n",
    "#     for f, D in features.items():\n",
    "#         # print(f\"Running for {f}\")\n",
    "        \n",
    "#         # print(f\"Train: {train}, Dev: {dev}, Test: {test}\")\n",
    "#         # # print(f\"Features: {D.columns}\")\n",
    "#         # print(f\"{D.shape}\")\n",
    "#         # print(f\"{D}\")\n",
    "        \n",
    "#         all_results.append(train_and_evaluate(train, dev, test, f, D))\n",
    "\n",
    "\n",
    "fig_3a = pd.DataFrame([res for sublist in all_results for res in sublist])\n",
    "fig_3a.to_csv('fig_3a.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date in features: year     2009\n",
      "month       1\n",
      "dtype: int64\n",
      "Max date in features: year     2017\n",
      "month      10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Min date in features:\", time_series[['year', 'month']].min())\n",
    "print(\"Max date in features:\", time_series[['year', 'month']].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is empty from (2017,7) to (2020,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import root_mean_squared_error, precision_recall_curve, auc\n",
    "\n",
    "test_splits = [\n",
    "    ((2010,7), (2011, 7)), \n",
    "    ((2011,7), (2012, 7)),\n",
    "    ((2012,7), (2013, 7)), \n",
    "    ((2013,7), (2014, 7)), \n",
    "    ((2014,7), (2015, 7)), \n",
    "    ((2015,7), (2016, 7)), \n",
    "    ((2016,7), (2017, 7)), \n",
    "    # ((2017,7), (2018, 7)),\n",
    "    # ((2018,7), (2019, 7)), \n",
    "    # ((2019,2), (2020, 2)),\n",
    "]\n",
    "train_splits_old = [\n",
    "    ((2009,7), (2010,4)),\n",
    "    ((2009,7), (2011,1)),\n",
    "    ((2009,7), (2011,10)),\n",
    "    ((2009,7), (2012,7)),\n",
    "    ((2009,7), (2013,7)),\n",
    "    ((2009,7), (2014,1)),\n",
    "    ((2009,7), (2015,1)),\n",
    "    ((2009,7), (2015,10)),\n",
    "    ((2009,7), (2016,10)),\n",
    "    ((2009,7), (2017,2))\n",
    "]\n",
    "dev_splits = [\n",
    "    ((2010,4),  (2010, 7)),\n",
    "    ((2011,1),  (2011, 7)),\n",
    "    ((2011,10), (2012, 7)),\n",
    "    ((2012,7),  (2013, 7)),\n",
    "    ((2013,4),  (2014, 7)),\n",
    "    ((2014,1),  (2015, 7)),\n",
    "    ((2015,1),  (2016, 7)),\n",
    "    ((2015,10), (2017, 7)),\n",
    "    # ((2016,10), (2018, 7)),\n",
    "    # ((2017,2), (2019, 2)),\n",
    "]\n",
    "\n",
    "# i just wanted to keep the training on a year or less. All values are taken from original train split but the start date is increased to reduce the time it is trained on. This is to stay consistent with testing across one year only - bilal.\n",
    "train_splits_new = [ \n",
    "    ((2009,7), (2011,1)),\n",
    "    ((2010,7), (2012,7)),\n",
    "    ((2011,7), (2013,7)),\n",
    "    ((2012,7), (2014,1)),\n",
    "    ((2013,7), (2015,10)),\n",
    "    ((2014,7), (2016,10)),\n",
    "    ((2015,7), (2017,2)),\n",
    "]\n",
    "\n",
    "train_splits =  train_splits_new\n",
    "\n",
    "# just like them we will evaluate three dufferent models, Random Forest, OLS and Lasso. Random Forest is a tree-based model, OLS is a linear regression model and Lasso is a linear regression model with L1 regularization\n",
    "models = {\n",
    "    'RF': RandomForestRegressor(max_features='sqrt', n_estimators=100, min_samples_split=0.5, min_impurity_decrease=0.001, random_state=0)\n",
    "    # 'RF': RandomForestRegressor(\n",
    "    #     max_features='sqrt',  # Keep this\n",
    "    #     n_estimators=500,  # Increase trees to reduce variance\n",
    "    #     min_samples_split=5,  # ðŸš¨ Fix this, should be an integer\n",
    "    #     min_samples_leaf=2,  # Helps prevent overfitting\n",
    "    #     max_depth=None,  # Allow full tree growth\n",
    "    #     bootstrap=True,  # Default setting, makes it more robust\n",
    "    #     random_state=0\n",
    "    # )\n",
    "    # 'OLS': LinearRegression(),\n",
    "    # 'Lasso': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "def get_agg_lagged_features(factors):\n",
    "    return [f\"{f}_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_province_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_country_{t}\" for f in factors for t in range(3, 9)]\n",
    "\n",
    "features = {\n",
    "    'traditional': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors],\n",
    "    \n",
    "    'news': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(news_factors)],\n",
    "    \n",
    "    'traditional+news': time_series[['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors + \n",
    "        get_agg_lagged_features(news_factors)]\n",
    "    \n",
    "    # 'expert': time_series[['fews_proj_near_3' ] + ['year', 'month']],\n",
    "    \n",
    "    # 'expert+traditional': time_series[ ['year', 'month']+ \n",
    "    #     ['fews_proj_near_3'] +\n",
    "    #     ['{}_{}'.format('fews_ipc', t) for t in range(3,21,3)] + \n",
    "    #     get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "    #     t_invariant_traditional_factors\n",
    "    # ],\n",
    "    # 'expert+news': time_series[ ['year', 'month'] +\n",
    "    #     ['fews_proj_near_3'] +\n",
    "    #     ['{}_{}'.format('fews_ipc', t) for t in range(3,21,3)] +\n",
    "    #     get_agg_lagged_features(news_factors)\n",
    "    # ],\n",
    "    # 'expert+traditional+news': time_series[ ['year', 'month'] +\n",
    "    #     ['fews_proj_near_3'] +\n",
    "    #     ['{}_{}'.format('fews_ipc', t) for t in range(3,21,3)] +\n",
    "    #     get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "    #     t_invariant_traditional_factors +\n",
    "    #     get_agg_lagged_features(news_factors)\n",
    "    # ]\n",
    "}\n",
    "\n",
    "labels_df = time_series[['fews_ipc', 'year', 'month']]\n",
    "\n",
    "def get_time_split(df, start, end):\n",
    "    return df[\n",
    "        (((df['year'] > start[0])) | ((df['year'] == start[0]) & (df['month'] >= start[1]))) &\n",
    "        (((df['year'] < end[0])) | ((df['year'] == end[0]) & (df['month'] <= end[1])))\n",
    "    ]\n",
    "\n",
    "thresholds = {'traditional': (2.236, 3.125), \n",
    "              'news': (1.907, 2.712), \n",
    "              'traditional+news': (2.105, 3.314),\n",
    "            #   'expert': (2, 3),\n",
    "            #   'expert+news': (1.912, 2.813),\n",
    "            #   'expert+traditional': (2.241, 3.132),\n",
    "            #   'expert+traditional+news': (2.172, 3.321)\n",
    "             }\n",
    "\n",
    "def train_and_evaluate(train, dev, test, f, D):\n",
    "    results = []\n",
    "    \n",
    "    X_train = get_time_split(D, train[0], train[1]).drop(columns=['year', 'month']).to_numpy()# not sure how okay it is to do fillna. When me and Bilal were running this we were getting the error that cannot run the model on NaN values. First we dropped na but this was causing the shape of the X_train to be different from the y_train. So we decided to fillna with 0. - aysha & bilal\n",
    "    \n",
    "    y_train = get_time_split(labels_df, train[0], train[1]).drop(columns=['year', 'month']).to_numpy().ravel()\n",
    "\n",
    "    nan_mask = np.isnan(X_train).any(axis=1)\n",
    "    X_train = X_train[~nan_mask]\n",
    "    y_train = y_train[~nan_mask]\n",
    "    \n",
    "    X_test = get_time_split(D, test[0], test[1]).drop(columns=['year', 'month']).to_numpy()\n",
    "    y_test = get_time_split(labels_df, test[0], test[1]).drop(columns=['year', 'month']).to_numpy().ravel()\n",
    "    nan_mask_test = np.isnan(X_test).any(axis=1)\n",
    "    X_test = X_test[~nan_mask_test]\n",
    "    y_test = y_test[~nan_mask_test]\n",
    "    \n",
    "    # check if X train is empty:\n",
    "    print(f\"Rows in X_train: {X_train.shape[0]} \\nRows in X_test: {X_test.shape[0]}\")\n",
    "    if X_train.shape[0] <= 0:\n",
    "        # print(f\"X train is empty for {f} from {train}\")\n",
    "        return results\n",
    "    if X_test.shape[0] <= 0:\n",
    "        # print(f\"X test is empty for {f} from {test}\")\n",
    "        return results\n",
    "       \n",
    "    # convert y_test into binary classification (1 if inside threshold, else 0)\n",
    "    lower, upper = thresholds[f]\n",
    "    y_test_binary = np.where((y_test >= lower) & (y_test <= upper), 1, 0)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        rmse = root_mean_squared_error(y_test, preds)\n",
    "        rmse = root_mean_squared_error(y_test, preds)\n",
    "\n",
    "        # stderr = np.std(y_test - preds) / np.sqrt(len(y_test))\n",
    "        # upper_bound = np.sqrt(rmse**2 + 1.96 * stderr)\n",
    "        # lower_bound = np.sqrt(rmse**2 - 1.96 * stderr)\n",
    "        # precision, recall, _ = precision_recall_curve(y_test_binary, preds)\n",
    "        # aucpr = auc(recall, precision)\n",
    "\n",
    "        results.append({\n",
    "            'method': name, 'split': test, 'features': f, \n",
    "            'rmse': rmse,\n",
    "            # 'rmse': rmse, 'lower_bound': lower_bound, 'upper_bound': upper_bound,\n",
    "            # 'aucpr': aucpr\n",
    "        })\n",
    "\n",
    "        # print(f\"Method: {name}, Split: {test}, Features: {f}, AUCPR: {aucpr:.4f}\")\n",
    "        # print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f} [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "        print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f}\")\n",
    "        \n",
    "    #     # completely removed the part where they were doing country-wise evaluation. Do not see point - aysha\n",
    "    \n",
    "    return results\n",
    "\n",
    "# run in parallel on 4 cpu cores/decrease this if you do not want ur system to crash (speaking from experience)\n",
    "# all_results = Parallel(n_jobs=4)(\n",
    "#     delayed(train_and_evaluate)(train, dev, test, f, D) for train, dev, test in zip(train_splits, dev_splits, test_splits) for f, D in features.items()\n",
    "# )\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for train, dev, test in zip(train_splits, dev_splits, test_splits):\n",
    "    for f, D in features.items():\n",
    "        try:\n",
    "            all_results.append(train_and_evaluate(train, dev, test, f, D))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {train} & {dev} & {test}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "fig_3a = pd.DataFrame([res for sublist in all_results for res in sublist])\n",
    "# fig_3a.to_csv('fig_3a.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method  features        \n",
       "RF      news                0.191303\n",
       "        traditional         0.176266\n",
       "        traditional+news    0.188119\n",
       "Name: rmse, dtype: float64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_3a.groupby(by=['method', 'features'])['rmse'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the train and the test dataset are the same:**\n",
    "```\n",
    "method  features        \n",
    "RF      news                0.099734\n",
    "        traditional         0.027667\n",
    "        traditional+news    0.096902\n",
    "Name: rmse, dtype: float64\n",
    "```\n",
    "**If we use their provided train-test split:**\n",
    "```\n",
    "method  features        \n",
    "RF      news                0.400070\n",
    "        traditional         0.134550\n",
    "        traditional+news    0.390469\n",
    "Name: rmse, dtype: float64\n",
    "```\n",
    "\n",
    "**If we use dev-test split:**\n",
    "```\n",
    "method  features        \n",
    "RF      news                0.388210\n",
    "        traditional         0.132620\n",
    "        traditional+news    0.364279\n",
    "Name: rmse, dtype: float64\n",
    "```\n",
    "**If we use train+dev-test split:**\n",
    "```\n",
    "method  features        \n",
    "RF      news                0.400070\n",
    "        traditional         0.134550\n",
    "        traditional+news    0.390469\n",
    "Name: rmse, dtype: float64\n",
    "```\n",
    "The reason why we get the same value regardless of whether we use the train-test split, or if we use train+dev-test split is because for each value of test split, a pair on the corresponding index of train value is taken to test the two together. this means if the test split is a list of 5 tuples, you will only train on the first 5 tuples of the train list and test them across the test list. Therefore making ur train sets longer does not by definition allow u to train on more data like that.\n",
    "\n",
    "Using the same model they have advertised, and mostly the same data, I tried to train and test on similar length of data. I did not modify their test splits at all. Without modifying their train splits a lot either, I came up with a new train split (mentioned in the code above) that gives the following error:\n",
    "```\n",
    "method  features        \n",
    "RF      news                0.135289\n",
    "        traditional         0.104422\n",
    "        traditional+news    0.132082\n",
    "Name: rmse, dtype: float64\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
