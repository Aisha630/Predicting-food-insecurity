{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š **Re-Implementation of \"Predicting Food Crises Using News Streams\"**\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” **Objective**\n",
    "\n",
    "This notebook aims to **reproduce and analyze** the methodology presented in the paper:\n",
    "\n",
    "ðŸ“„ **Paper:** [Predicting food crises using news streams](https://www.science.org/doi/10.1126/sciadv.abm3449)  \n",
    "ðŸ“Š **Dataset:** [Harvard Dataverse Repository](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/CJDWUW)  \n",
    "ðŸ“œ **Original Code & Methods:** [GitHub - Regression Modeling (Step 5)](https://github.com/philippzi98/food_insecurity_predictions_nlp/blob/main/Step%205%20-%20Regression%20Modelling/README.md)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ›  **Methodology**\n",
    "\n",
    "This implementation follows the **key steps** outlined in the paper to predict **food insecurity crises** using a combination of:\n",
    "1ï¸âƒ£ **Traditional Risk Factors** (conflict, climate, food prices, etc.)  \n",
    "2ï¸âƒ£ **News-Based Indicators** (text feature frequencies from news articles)  \n",
    "3ï¸âƒ£ **Lagging & Aggregation** (temporal dependencies at district, province, and country levels)  \n",
    "4ï¸âƒ£ **Machine Learning Models** (Random Forest, OLS, Lasso)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”— **Reference Materials**\n",
    "\n",
    "ðŸ“„ **Supplementary Material:** Available in `supplemental_material_from_paper.pdf`  \n",
    "ðŸ“Š **Datasets Used:**\n",
    "\n",
    "- `time_series_with_causes_zscore_full.csv` (Main dataset with time-series features)\n",
    "- `famine-country-province-district-years-CS.csv` (Food insecurity classification)\n",
    "- `matching_districts.csv` (Geographical standardization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“šðŸ”§ Import Libraries\n",
    "\n",
    "In this notebook, we will use uv to manage our Python environment and packages efficiently. uv is a modern and fast package manager that simplifies virtual environment creation, and dependency installation. We will create a virtual environment, install necessary libraries, and ensure our environment stays consistent across different setups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncoment the below cell to install `uv` if you have not already. You can also install it trhiugh `pip` by running `!pip install uv` but this will be within your current python environment and not globally.\n",
    "\n",
    "# !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "# !uv venv world-bank\n",
    "# !source world-bank/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# Run the line below to create the working conda environment\n",
    "# conda env create -f environment.yml \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "from fuzzywuzzy import fuzz\n",
    "import math\n",
    "import polars as pl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have the data downloaded and extracted\n"
     ]
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?id=1YoQ1hz9RlaLr2xW3KoKCfJPyyO2PErym\"\n",
    "output = \"data.zip\"\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    gdown.download(url, output, quiet=False) \n",
    "    zipfile.ZipFile('data.zip', 'r').extractall()\n",
    "else:\n",
    "    print(\"You already have the data downloaded and extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Load and Clean Data\n",
    "\n",
    "**Understanding the Time-Series Dataset & Column Selection**\n",
    "\n",
    "This dataset contains **district-level time-series data** on food insecurity risk factors, including:\n",
    "\n",
    "- **ðŸ“… Temporal Information:** `year`, `month`, `year_month`\n",
    "- **ðŸ“ Geographical Identifiers:** `admin_code`, `admin_name`, `province`, `country`\n",
    "- **ðŸŒ Traditional Risk Factors:** Climate (`rain_mean`, `ndvi_mean`), conflict (`acled_count`), food prices (`p_staple_food`)\n",
    "- **ðŸ“° News-Based Indicators:** Proportions of news articles mentioning crisis-related keywords (`conflict_0`, `famine_0`, etc.)\n",
    "- **ðŸ“‰ Food Insecurity Label:** `fews_ipc` (Integrated Phase Classification)\n",
    "\n",
    "ðŸ”¥ **Columns We Will Drop & Why**\n",
    "âœ” **Redundant Aggregations:** `_1`, `_2` columns (province & country-level values) since we will recompute aggregations from scratch anyways.  \n",
    "âœ” **Unnamed/Index Columns:** `Unnamed: 0` as it is unnecessary. It is just a duplicate of default index.\n",
    "âœ” **Unnecessary Identifiers:** If `admin_code` and `admin_name`, after matching these to `matching_districts.csv`, we can drop them.\n",
    "\n",
    "---\n",
    "\n",
    "> âš ï¸ **NOTE:**  \n",
    "> For a detailed explanation of the dataset and features, refer to the [`explore_time_series.ipynb`](./explore_time_series.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pl.read_csv('./data/time_series_with_causes_zscore_full.csv')\n",
    "admins = pl.read_csv('./data/famine-country-province-district-years-CS.csv', schema_overrides={\"CS\": pl.Float64})\n",
    "valid_matching = pl.read_csv('./data/matching_districts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 532)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>index</th><th>country</th><th>admin_code</th><th>admin_name</th><th>centx</th><th>centy</th><th>year_month</th><th>year</th><th>month</th><th>fews_ipc</th><th>fews_ha</th><th>fews_proj_near</th><th>fews_proj_near_ha</th><th>fews_proj_med</th><th>fews_proj_med_ha</th><th>ndvi_mean</th><th>ndvi_anom</th><th>rain_mean</th><th>rain_anom</th><th>et_mean</th><th>et_anom</th><th>acled_count</th><th>acled_fatalities</th><th>p_staple_food</th><th>area</th><th>cropland_pct</th><th>pop</th><th>ruggedness_mean</th><th>pasture_pct</th><th>change_fews</th><th>land seizures_0</th><th>land seizures_1</th><th>land seizures_2</th><th>slashed export_0</th><th>slashed export_1</th><th>slashed export_2</th><th>&hellip;</th><th>authoritarian_2</th><th>dictators_0</th><th>dictators_1</th><th>dictators_2</th><th>clans_0</th><th>clans_1</th><th>clans_2</th><th>gastrointestinal_0</th><th>gastrointestinal_1</th><th>gastrointestinal_2</th><th>terrorist_0</th><th>terrorist_1</th><th>terrorist_2</th><th>warlord_0</th><th>warlord_1</th><th>warlord_2</th><th>d&#x27;etat_0</th><th>d&#x27;etat_1</th><th>d&#x27;etat_2</th><th>overthrow_0</th><th>overthrow_1</th><th>overthrow_2</th><th>convoys_0</th><th>convoys_1</th><th>convoys_2</th><th>carbon_0</th><th>carbon_1</th><th>carbon_2</th><th>mayhem_0</th><th>mayhem_1</th><th>mayhem_2</th><th>dehydrated_0</th><th>dehydrated_1</th><th>dehydrated_2</th><th>mismanagement_0</th><th>mismanagement_1</th><th>mismanagement_2</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>30</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2009_07&quot;</td><td>2009</td><td>7</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.106035</td><td>106.547146</td><td>0.353588</td><td>-0.070848</td><td>0.191125</td><td>-0.073903</td><td>0</td><td>0</td><td>1.065669</td><td>54174.53381</td><td>1.417796</td><td>1.241226e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>-0.765667</td><td>-0.426667</td><td>0.886</td><td>0.597667</td><td>-0.987</td><td>1.449333</td><td>&hellip;</td><td>0.75</td><td>0.496</td><td>1.557333</td><td>1.252333</td><td>0.827</td><td>-0.035667</td><td>-0.02</td><td>-0.192</td><td>0.281667</td><td>-0.259667</td><td>-0.284333</td><td>1.626</td><td>0.532667</td><td>-0.668667</td><td>1.497333</td><td>-0.794667</td><td>0.647333</td><td>1.652333</td><td>-0.029</td><td>-0.891333</td><td>0.848333</td><td>1.472667</td><td>0.112667</td><td>-0.887</td><td>-0.963667</td><td>1.265333</td><td>-0.493667</td><td>1.053</td><td>0.667</td><td>-0.171</td><td>-0.833</td><td>0.173667</td><td>0.168</td><td>1.284667</td><td>-0.073</td><td>-0.427667</td><td>0.668333</td></tr><tr><td>1</td><td>33</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2009_10&quot;</td><td>2009</td><td>10</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.103009</td><td>106.034013</td><td>0.409304</td><td>-0.116134</td><td>0.69447</td><td>0.225598</td><td>0</td><td>0</td><td>1.100531</td><td>54174.53381</td><td>1.417796</td><td>1.241226e6</td><td>101047.1587</td><td>16.246279</td><td>1.0</td><td>-0.556272</td><td>-0.791605</td><td>-0.903605</td><td>-0.933739</td><td>-0.645739</td><td>-0.918405</td><td>&hellip;</td><td>-1.027332</td><td>-0.665846</td><td>-0.591846</td><td>-1.039846</td><td>-0.756904</td><td>-0.590571</td><td>-1.234904</td><td>-0.545727</td><td>-0.474394</td><td>-0.841394</td><td>-1.037016</td><td>-0.917683</td><td>-0.787683</td><td>-0.811291</td><td>-0.713958</td><td>-1.257625</td><td>-0.850261</td><td>-0.831261</td><td>-0.759594</td><td>-0.948892</td><td>-1.198892</td><td>-0.883225</td><td>-0.728972</td><td>-1.203638</td><td>-0.874305</td><td>-0.765146</td><td>-1.141479</td><td>-0.660812</td><td>-0.63658</td><td>-0.520247</td><td>-0.782913</td><td>-0.671587</td><td>-0.612254</td><td>-0.926921</td><td>-0.510467</td><td>-0.625133</td><td>-0.452467</td></tr><tr><td>2</td><td>36</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2010_01&quot;</td><td>2010</td><td>1</td><td>2.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.1096</td><td>111.433187</td><td>3.894158</td><td>-2.333251</td><td>3.441319</td><td>-1.450951</td><td>0</td><td>0</td><td>0.98839</td><td>54174.53381</td><td>1.417796</td><td>1.280853e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>-0.006667</td><td>0.431</td><td>0.67</td><td>0.755667</td><td>-0.263</td><td>-0.574</td><td>&hellip;</td><td>-0.307333</td><td>0.010333</td><td>0.485</td><td>-0.820333</td><td>1.460333</td><td>-0.351</td><td>0.817667</td><td>1.506333</td><td>1.484667</td><td>-0.378667</td><td>0.455</td><td>-0.871</td><td>0.766</td><td>1.595667</td><td>0.023667</td><td>-0.968667</td><td>0.571667</td><td>-0.837333</td><td>0.444667</td><td>0.279</td><td>0.078</td><td>0.614333</td><td>-0.868333</td><td>-0.598</td><td>1.316</td><td>0.058333</td><td>1.368</td><td>-0.134333</td><td>1.447667</td><td>-0.844333</td><td>0.778667</td><td>-0.676</td><td>-0.689667</td><td>0.293333</td><td>0.530333</td><td>-0.471333</td><td>0.955333</td></tr><tr><td>3</td><td>39</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2010_04&quot;</td><td>2010</td><td>4</td><td>2.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.111599</td><td>94.212242</td><td>1.609664</td><td>-0.788739</td><td>1.851542</td><td>-0.771469</td><td>0</td><td>0</td><td>0.992492</td><td>54174.53381</td><td>1.417796</td><td>1.280853e6</td><td>101047.1587</td><td>16.246279</td><td>-1.0</td><td>-0.193697</td><td>-0.613697</td><td>-0.307364</td><td>0.311536</td><td>0.337869</td><td>-0.524797</td><td>&hellip;</td><td>-0.369667</td><td>-0.090077</td><td>0.224923</td><td>-0.58841</td><td>-0.616381</td><td>-0.605381</td><td>-0.114381</td><td>-0.79397</td><td>0.29903</td><td>-0.63397</td><td>-0.722159</td><td>-0.130159</td><td>-0.123825</td><td>-0.130521</td><td>-0.578521</td><td>0.090146</td><td>0.04763</td><td>0.137297</td><td>-0.603036</td><td>0.362613</td><td>0.231613</td><td>0.018279</td><td>0.480986</td><td>-0.427347</td><td>-0.121014</td><td>0.026073</td><td>0.165406</td><td>-0.326927</td><td>-0.594877</td><td>0.16479</td><td>-0.90521</td><td>-0.62054</td><td>0.165794</td><td>0.045794</td><td>-1.0116</td><td>-0.8106</td><td>-0.2056</td></tr><tr><td>4</td><td>42</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2010_07&quot;</td><td>2010</td><td>7</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.096943</td><td>97.411677</td><td>0.3938336</td><td>-0.030602</td><td>0.291468</td><td>0.026441</td><td>0</td><td>0</td><td>1.024889</td><td>54174.53381</td><td>1.417796</td><td>1.280853e6</td><td>101047.1587</td><td>16.246279</td><td>1.0</td><td>-0.787272</td><td>-0.725605</td><td>-0.879272</td><td>-0.598072</td><td>-0.803072</td><td>-0.817739</td><td>&hellip;</td><td>-0.748332</td><td>-0.611846</td><td>-0.511179</td><td>-0.470512</td><td>-0.791904</td><td>-1.053238</td><td>-0.653238</td><td>-0.509394</td><td>-0.462727</td><td>-0.856727</td><td>-0.69435</td><td>-1.102683</td><td>-1.13235</td><td>-1.215958</td><td>-0.832291</td><td>-0.948291</td><td>-0.865261</td><td>-0.812261</td><td>-0.645928</td><td>-1.119225</td><td>-0.977558</td><td>-0.758892</td><td>-1.060638</td><td>-0.876972</td><td>-1.210305</td><td>-0.673479</td><td>-1.090479</td><td>-1.085146</td><td>-0.709913</td><td>-0.867913</td><td>-0.770247</td><td>-0.787921</td><td>-0.974587</td><td>-0.946921</td><td>-0.611133</td><td>-0.7098</td><td>-0.6228</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 532)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚     â”† index â”† country     â”† admin_code â”† â€¦ â”† dehydrated_ â”† mismanageme â”† mismanagem â”† mismanagem â”‚\n",
       "â”‚ --- â”† ---   â”† ---         â”† ---        â”†   â”† 2           â”† nt_0        â”† ent_1      â”† ent_2      â”‚\n",
       "â”‚ i64 â”† i64   â”† str         â”† i64        â”†   â”† ---         â”† ---         â”† ---        â”† ---        â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”† f64         â”† f64         â”† f64        â”† f64        â”‚\n",
       "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0   â”† 30    â”† Afghanistan â”† 202        â”† â€¦ â”† 1.284667    â”† -0.073      â”† -0.427667  â”† 0.668333   â”‚\n",
       "â”‚ 1   â”† 33    â”† Afghanistan â”† 202        â”† â€¦ â”† -0.926921   â”† -0.510467   â”† -0.625133  â”† -0.452467  â”‚\n",
       "â”‚ 2   â”† 36    â”† Afghanistan â”† 202        â”† â€¦ â”† 0.293333    â”† 0.530333    â”† -0.471333  â”† 0.955333   â”‚\n",
       "â”‚ 3   â”† 39    â”† Afghanistan â”† 202        â”† â€¦ â”† 0.045794    â”† -1.0116     â”† -0.8106    â”† -0.2056    â”‚\n",
       "â”‚ 4   â”† 42    â”† Afghanistan â”† 202        â”† â€¦ â”† -0.946921   â”† -0.611133   â”† -0.7098    â”† -0.6228    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_variant_traditional_factors = [ 'p_staple_food']\n",
    "t_variant_traditional_factors = ['ndvi_mean', 'ndvi_anom', 'rain_mean', 'rain_anom', 'et_mean', 'et_anom', \n",
    "                                    'acled_count', 'acled_fatalities', 'p_staple_food']\n",
    "t_invariant_traditional_factors = ['area', 'cropland_pct', 'pop', 'ruggedness_mean', 'pasture_pct']\n",
    "news_factors = [name for name in time_series.columns if '_0' in name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'land seizures_0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_factors[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential extra columns ['', 'admin_code', 'admin_name', 'centx', 'centy', 'change_fews', 'country', 'fews_ha', 'fews_ipc', 'fews_proj_med', 'fews_proj_med_ha', 'fews_proj_near', 'fews_proj_near_ha', 'index', 'month', 'year', 'year_month']\n"
     ]
    }
   ],
   "source": [
    "potential_extra_cols = set(time_series.columns) - set(t_variant_traditional_factors) - set(t_invariant_traditional_factors) - set(news_factors)\n",
    "potential_extra_cols = [col for col in potential_extra_cols if not col.endswith(('_1', '_2', '_3'))]\n",
    "print(\"Potential extra columns\", sorted(potential_extra_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Admin Level Mapping: Standardizing Geographical Identifiers\n",
    "\n",
    "In this section, we will **map and standardize** the `admin_code` and `admin_name` fields to their corresponding **district, province, and country names**. This step is **crucial** for ensuring **consistency** across different datasets and enabling **accurate aggregations** at multiple administrative levels.\n",
    "\n",
    "ðŸ›  **Why is Admin Level Mapping Important?**\n",
    "âœ… Different datasets may use **slightly different spellings or formats** for district names.  \n",
    "âœ… Some district names might be **missing or misspelled**, requiring standardization.  \n",
    "âœ… We need to **match and align** district names across various sources before aggregating at **province and country levels**.  \n",
    "âœ… Proper mapping allows us to **merge datasets correctly** without losing information.  \n",
    "\n",
    "ðŸ“Œ **Steps in Admin Mapping**\n",
    "1ï¸âƒ£ **Load the `matching_districts.csv` file**, which provides the mapping between different district name variations.  \n",
    "2ï¸âƒ£ **Identify missing or unmatched `admin_name` values** and find their closest matches using fuzzy matching techniques.  \n",
    "3ï¸âƒ£ **Ensure that each `admin_code` uniquely maps to one `district`, `province`, and `country`.**  \n",
    "4ï¸âƒ£ **Replace inconsistent names** in the dataset with their standardized versions.  \n",
    "5ï¸âƒ£ **Aggregate data at the `province` and `country` levels** after ensuring all districts are correctly mapped.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(admins.select(pl.col(\"country\").n_unique()).to_numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'country', 'district', 'year', 'month', 'CS', 'province']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admins.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_names = time_series['admin_name'].unique()\n",
    "districts = admins['district'].unique()\n",
    "provinces = admins['province'].unique()\n",
    "countries = admins['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142 4113 474 39\n",
      "369\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "print (len(admin_names), len(districts), len(provinces), len(countries))\n",
    "print (len(set(admin_names).difference(districts)))\n",
    "missing_admin_names = set(admin_names).difference(districts)\n",
    "print (len(missing_admin_names.difference(provinces)))\n",
    "missing_admin_names = missing_admin_names.difference(provinces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy String Matching for Missing Names\n",
    "\n",
    "The function uses **fuzzy string matching** to find the best approximate matches for missing administrative names (e.g., districts and provinces). \n",
    "\n",
    "- Finds the **best matching district/province** for each missing name.\n",
    "- Uses **fuzzy string matching** to calculate the similarity between missing names and known names.\n",
    "- Returns a dictionary that maps each missing name to its closest match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gourma-Rharous Gourma Ghor\n",
      "West Harerge West Hararge West Darfur\n",
      "MPongwe Mpongwe Bong\n",
      "Lulua Luilu Lualaba\n",
      "KantchÃ© Kantche Kano\n",
      "Bale.1 Bale Bay\n",
      "Valliere Vallieres Niger\n",
      "Mwingi Mwingi West Migori\n",
      "Abu Hamad Abu Hamed Hilmand\n",
      "Sheikh Jebrat El Sheikh Sahel\n",
      "Beni San Benito Benue\n",
      "Al Kurumik Qulansiyah wa `Abd Al Kuri Ituri\n",
      "Gucha Kabuchai Ahuachapan\n",
      "La PendÃ© La Pende Lac\n",
      "Al Faw El Faw Al Jawf\n",
      "Central Kisii Kiti Central\n",
      "North Gonder North Gondar North\n",
      "South Khartoum Khartoum Khartoum\n",
      "Anse-D'Ainault `Ain Abia\n",
      "AguiÃ© Aguie Bangui\n",
      "Guji Gujii Guidimaka\n",
      "Mashra'ah wa Hadnan Mashra`ah wa Hadnan Kankan\n",
      "North al Gazera Ganze North\n",
      "Id El Ghanem Ganye Kanem\n",
      "Lac-LÃ©rÃ© Lac-Lere Lac\n",
      "Nandi South Nnewi South Nandi\n",
      "Al Gutaina El Gutaina Rutana\n",
      "South Gonder South Gondar Sud\n",
      "Sharg En Nile Sahar Niger\n",
      "Mangwe (South) Mangwe Southern\n",
      "Croix-Des-Bouquets Bo Ouest\n",
      "Gweru Gweru Rural Meru\n",
      "Eastern Tigray Miga Eastern\n",
      "Chiredzi Chiredzi Rural Moyen-Chari\n",
      "Port-Salut Port Salut Salamat\n",
      "Kwekwe Kwekwe Rural Kwale\n",
      "UMP Keur Macene Ouham-Pende\n",
      "Hareri Harari Harari\n",
      "Hamashkorieb Hamashkoreib Ghor\n",
      "Zvishavane Zvishavane Urban Kanem\n",
      "Nahr Atbara Atbara Abia\n",
      "Caynabo Caynaba Gao\n",
      "Kabkabiya Kebkabiya Abia\n",
      "Sa'dah As Saddah Sa`dah\n",
      "Kibale Kibaale Kidal\n",
      "Ville de Tahoua Tahoua Tahoua\n",
      "At Ta'izziyah At Ta`izziyah Ta'izz\n",
      "Mayo Boneye Bo Boke\n",
      "Hwange Hwange Rural Kwango\n",
      "Meru North Meru Meru\n",
      "Al Fushqa Al Husha' Arusha\n",
      "Khartoum Bahri Khartoum Khartoum\n",
      "Keiyo Keiyo North Oyo\n",
      "Beitbridge Beitbridge Rural Uige\n",
      "Rab Dhuure Rabdhuure Central Darfur\n",
      "Bura' Bura Guera\n",
      "Al Jabalian Jaba Al Jawf\n",
      "Teso Teso North El Progreso\n",
      "GothÃ¨ye Gotheye Gao\n",
      "Damagaram Takaya Takaya Mara\n",
      "Majang Lal Wa Sarjangal Mahajanga\n",
      "Ad Damazin Daman Adamawa\n",
      "Al Geneina El Geneina Benue\n",
      "Lafon Lopa/Lafon Lac\n",
      "Ville de Maradi Maridi Maradi\n",
      "Koibatek Kibra Kogi\n",
      "Tesker Tasker Western Equatoria\n",
      "Gebiley Gabiley White Nile\n",
      "Muranga Muhanga Murang'a\n",
      "Kuria Kuria West Ituri\n",
      "En Nuhud Al Nuhud Sud\n",
      "Bulo Burto Bulo-Burte Borno\n",
      "Tanganyka Tanga Tanga\n",
      "Maragua Maragwa Mara\n",
      "Bukavu City of Bukavu Rukwa\n",
      "Addabah Rabah Assaba\n",
      "Kolwezi City of Kolwezi Jonglei\n",
      "Ceca La Source Cerca La Source Lac\n",
      "Butere Mumias Butere Muyinga\n",
      "Belet Xaawo Beled-Xaawo Gao\n",
      "Segen Peoples' Segen Benue\n",
      "Buret Bureti Kasai-Oriental\n",
      "Amanat Al 'Asimah Omna Amanat Al `Asimah\n",
      "Burtinle Butinle Iilemi triangle\n",
      "Mwene-Ditu City of Mwene-Ditu Kitui\n",
      "Trou Du Nord Trou du Nord Nord\n",
      "Mt Elgon Mt. Elgon Bong\n",
      "Shar'ab Ar Rawnah Shar`ab Ar Rawnah Mara\n",
      "Kabia Mambah Kaba Kajiado\n",
      "Al Rahd El Rahad Al Mahwit\n",
      "Barh-KÃ´h Barh-Koh Bari\n",
      "Kasai.1 Kpaai Kasai\n",
      "Busia.1 Busia Busia\n",
      "Shar'ab As Salam Shar`ab As Salam Sila\n",
      "Western Tigray Miga Tigray\n",
      "Bindura Bindura Urban Cabinda\n",
      "Al Gadaref Gada Gedaref\n",
      "Likasi City of Likasi Likouala\n",
      "Chiengi Chienge Muchinga\n",
      "Abu Jubaiyah Juba Raymah\n",
      "Bandarbeyla Bandar Beyla Mbeya\n",
      "Mbuji-Mayi City of Mbuji-Mayi Bay\n",
      "Hirat Wag Himra Hiiraan\n",
      "TillabÃ©ri Tillaberi Commune Tillaberi\n",
      "Al Fasher El Fasher Al Mahrah\n",
      "Merawi Al Marawi`ah Meru\n",
      "East al Gazera Ganze Gaza\n",
      "Al Wazi'iyah Al Wazi`iyah Siaya\n",
      "Um Badda Fada Bay\n",
      "Baydhaba Baydhabo Bay\n",
      "Banadir Dandi Banaadir\n",
      "Sar-e-Pul Sharak-e-Hayratan Sari Pul\n",
      "North Shewa(R4) North Shewa North\n",
      "Belet Weyne Bale Benue\n",
      "Mole Saint Nicolas Mole Saint-Nicolas White Nile\n",
      "Bulilima (North) Bulilima North\n",
      "Lubumbashi City of Lubumbashi Lahij\n",
      "Nandi North Nnewi North Nandi\n",
      "Amran `Amran `Amran\n",
      "GourÃ© Govuro Ghor\n",
      "Al Ma'afir Al Ma`afir Mara\n",
      "Al Roseires El Roseires Zaire\n",
      "Mawza' Mawza` Gaza\n",
      "MaÃ¯nÃ© Soroa Maine Soroa Sool\n",
      "Chipinge Chipinge Rural Uige\n",
      "Al Galabat Eastern El Galabat Gaza\n",
      "Kas Kasarani Kasai-Oriental\n",
      "North Western Tigray Miga North\n",
      "Sami' Sami` Lomami\n",
      "Port-Au-Prince Port au Prince Ituri\n",
      "Kadoma Kadoma Urban Kano\n",
      "Jebrat al Sheikh Jebrat El Sheikh Herat\n",
      "Butembo City of Butembo Kemo\n",
      "Gedio Gedeo Gedo\n",
      "Kelem Wellega Kelem Kanem\n",
      "Doolo Doolow Sool\n",
      "Karary Karari Kwara\n",
      "Gonave La Gonave Gao\n",
      "Barh El Gazel Ouest Barh el Gazel Ouest Ouest\n",
      "IllÃ©la Illela Solola\n",
      "Tayeeglow Tiyeglow Bay\n",
      "Thika Thika Town Tshuapa\n",
      "East Harerge East Hararge East Darfur\n",
      "Komonjdjari Komondjari Kemo\n",
      "Gwanda Gwanda Urban Cabinda\n",
      "Belbedji Belleh Benue\n",
      "TÃ©ra Tarauni Taraba\n",
      "Chegutu Chegutu Rural Hodh ech Chargui\n",
      "Acul Du Nord Acul du Nord Nord\n",
      "Trans Mara Marka Mara\n",
      "Ad Damer Ad Dihar Dhamar\n",
      "Mayo-Lemi Mayo-Lemie Gao\n",
      "Sharq al Gazera Ganze Gaza\n",
      "Rachuonyo Karachuonyo Lac\n",
      "Sheikan Shiekan Shinyanga\n",
      "Al Marawi'ah Marawi Mara\n",
      "Djourouf Al Ahmar Sourou Dhamar\n",
      "Mayo Binder Mayo-Binder Zinder\n",
      "Al Gash Al Hashwah Al Mahwit\n",
      "Taleex Talex Woqooyi Galbeed\n",
      "Saint-Raphael Saint Raphael Batha\n",
      "As Salam Shar`ab As Salam Dar es Salaam\n",
      "Wardi Hawar Wadi Hawar Bari\n",
      "Iriba Marimba Ma'rib\n",
      "Awi/Agew Awi Uige\n",
      "Sud-Kivu Kiru Sud\n",
      "Kananga City of Kananga Haut-Katanga\n",
      "South al Gazera Ganze Gaza\n",
      "Southern Tigray Soum Southern\n",
      "Sowdari Sodari Bari\n",
      "North Shewa(R3) North Shewa North\n",
      "Barh El Gazel Sud Barh el Gazel Sud Sud\n",
      "Grande Riviere Du Nord Grande Riviere du Nord Nord\n",
      "Owdweyne Oodweyne Benue\n",
      "FilinguÃ© Filingue Enugu\n",
      "Marakwet Marakwet West Elgeyo-Marakwet\n",
      "Meru South Meru Meru\n",
      "Shabelle Shebelle Lower Shabelle\n",
      "Seteet Seke Tete\n",
      "Mutare Mutare Urban Mtwara\n",
      "Saint Louis Du Nord Saint-Louis du Nord Nord\n",
      "Adan Far` Al `Udayn `Adan\n",
      "Adan Yabaal Aadan Yabaal `Adan\n",
      "Al Kamlin Kamina Kigali\n",
      "Kisangani City of Kisangani Tanga\n",
      "Kolwezi.1 City of Kolwezi Kolda\n",
      "Selti Selibaby Copperbelt\n",
      "Barh El Gazel Nord Barh el Gazel Nord Nord\n",
      "Cayes Les Cayes Kayes\n",
      "Anse-A-Veau `Ans Lamu\n",
      "Siti Ekiti West Simiyu\n",
      "Ad Dinder Ad Dis Zinder\n",
      "Um Al Gura Guera Guera\n",
      "Mbeere Mbeere South Mbeya\n",
      "Nord-Kivu Kiru Nord\n",
      "Tulus Kulbus Retalhuleu\n",
      "La Nya PendÃ© La Nya Lac\n",
      "Lughaye Lughaya Kayes\n",
      "Galdogob Goldogob Edo\n",
      "Ndjamena Same N'Djamena\n",
      "KT Keur Macene Kedougou\n",
      "Ghebeish Nesh Abyei\n",
      "Kajo-keji Kajo-Keji Boke\n",
      "Al Mahagil Mahagi Al Mahwit\n",
      "Baw Barh el Gazel Sud Baghlan\n",
      "Gokwe South Gokwe South Urban Southern\n",
      "Zallingi Zalingei Singida\n",
      "Al Deain Doedain Al Mahwit\n",
      "Kindu City of Kindu Kindia\n",
      "Wedza Wedjah Federal Capital Territory\n",
      "Berber Berbera N'Zerekore\n",
      "Ad Dali' Ad Dali` Ad Dali`\n",
      "Um Kadada Um Keddada Kandahar\n",
      "Special Woreda Yei Nord\n",
      "Balleyara Nara Mara\n",
      "Ad Douiem El Douiem Ad Dali`\n",
      "Addis Adaba Alaba Addis Ababa\n",
      "Agnuak Awgu Ouaka\n",
      "Meru Central Meru Central\n",
      "Port De Paix Port de Paix Pwani\n",
      "BankilarÃ© Bankilare Sila\n",
      "Shurugwi Shurugwi Urban Sud\n",
      "Goma Gomal Toamasina\n",
      "Bossaso Bo Gao\n",
      "Wadi Halfa Halfa Wadi Fira\n",
      "Nyala.1 Nyala Nyandarua\n",
      "MangalmÃ© Mangalme Tanga\n",
      "Ville de Niamey Same Niamey\n",
      "Laasqoray Lau Lac\n",
      "Ville de Zinder Zing Zinder\n"
     ]
    }
   ],
   "source": [
    "def find_matching(missing, names):\n",
    "    matching_districts = {}\n",
    "    for m in missing:\n",
    "        max_overlap = 0\n",
    "        nearest_d = None\n",
    "        for d in names:\n",
    "            d = str(d)\n",
    "            dist = fuzz.partial_ratio(m, d)\n",
    "            if dist > max_overlap:\n",
    "                max_overlap = dist\n",
    "                nearest_d = d\n",
    "        matching_districts[m] = nearest_d\n",
    "    return matching_districts\n",
    "\n",
    "\n",
    "matching = find_matching(missing_admin_names, districts)\n",
    "matching_p = find_matching(missing_admin_names, provinces)\n",
    "\n",
    "# manually verify matching and update\n",
    "for k in matching.keys():\n",
    "    print (k, matching[k], matching_p[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Decoding\n",
    "\n",
    "`to_ascii_escaped(s)`: Converts a Unicode string to an ASCII-safe representation using **unicode-escape**.\n",
    "\n",
    "`from_ascii_escaped(escaped)`: Converts the escaped ASCII string back into its original Unicode form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ascii_escaped(s):\n",
    "    \"\"\"\n",
    "    Convert a Unicode string to an ASCII-safe string using unicode-escape.\n",
    "    This will replace non-ASCII characters with their escape sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8')\n",
    "    # Using 'unicode-escape' encoding produces a bytes object,\n",
    "    # then decode it to get an ASCII string.\n",
    "    return s.encode('unicode-escape').decode('ascii')\n",
    "\n",
    "def from_ascii_escaped(escaped):\n",
    "    \"\"\"\n",
    "    Convert the ASCII-escaped string back to the original Unicode string.\n",
    "    \"\"\"\n",
    "    # Encode the ASCII string to bytes, then decode using 'unicode-escape'\n",
    "    return escaped.encode('ascii').decode('unicode-escape')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Province for a Given District or Province\n",
    "\n",
    "`find_province(x)`, finds the **province** corresponding to a given administrative name. It accounts for:\n",
    "- **Direct Lookups** (Exact match in known district/province lists)\n",
    "- **Fuzzy Matching** (Using ASCII-safe transformation for inconsistent text encoding)\n",
    "- **Validation Against a Predefined Mapping (`valid_matching`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matched globally\n",
    "matched = valid_matching['missing'].unique()\n",
    "\n",
    "def to_ascii_escaped(s):\n",
    "    \"\"\"\n",
    "    Convert a Unicode string to an ASCII-safe string using unicode-escape.\n",
    "    This will replace non-ASCII characters with their escape sequences.\n",
    "    \"\"\"\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8')\n",
    "    return s.encode('unicode-escape').decode('ascii')\n",
    "\n",
    "def find_province(x):\n",
    "    try:\n",
    "        # Ensure x is a Unicode string.\n",
    "        if isinstance(x, bytes):\n",
    "            x = x.decode('utf-8')\n",
    "        \n",
    "        if x in districts:\n",
    "            return admins.filter(pl.col('district') == x).select('province').to_series()[0]\n",
    "        elif x in provinces:\n",
    "            return x\n",
    "\n",
    "        escaped_x = to_ascii_escaped(x)\n",
    "\n",
    "        if escaped_x in matched:\n",
    "            # print(f\"Found {escaped_x} in matched\")\n",
    "            v = valid_matching.filter(pl.col('missing') == escaped_x)\n",
    "            # print(f\"Matched value: {v}\")\n",
    "            if v['match'][0] == 'district':\n",
    "                x2 = v['district'][0]\n",
    "                return admins.filter(pl.col('district') == x2)['province'][0]\n",
    "            elif v['match'][0] == 'province':\n",
    "                return v['province'][0] \n",
    "        \n",
    "        raise Exception(\"No matching province found\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Province not found for: {x} ({e})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Admin Names with Accented Characters and Mapping to Provinces\n",
    "\n",
    "Maps `admin_names` to provinces using the `find_province(a)` function.  \n",
    "If a **direct lookup fails**, it tries to handle cases where the **admin name contains accented characters** (`Ã©`, `Ã¨`, `Ã´`) ->  (encoding decoding issues resolved through directly replacing these with 'e' or 'o', leads to finding a valid match). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with: Barh-KÃ´h\n",
      "Replaced 'Barh-KÃ´h' with 'Barh-Koh', found province: Moyen-Chari\n",
      "Error with: TillabÃ©ri\n",
      "Replaced 'TillabÃ©ri' with 'Tillaberi', found province: Tillaberi\n",
      "Error with: Lac-LÃ©rÃ©\n",
      "Replaced 'Lac-LÃ©rÃ©' with 'Lac-Lere', found province: Mayo-Kebbi Ouest\n",
      "Error with: FilinguÃ©\n",
      "Replaced 'FilinguÃ©' with 'Filingue', found province: Tillaberi\n",
      "Error with: AguiÃ©\n",
      "Replaced 'AguiÃ©' with 'Aguie', found province: Maradi\n",
      "Error with: KantchÃ©\n",
      "Replaced 'KantchÃ©' with 'Kantche', found province: Zinder\n",
      "Error with: La Nya PendÃ©\n",
      "Replaced 'La Nya PendÃ©' with 'La Nya Pende', found province: Logone Oriental\n",
      "Error with: TÃ©ra\n",
      "Replaced 'TÃ©ra' with 'Tera', found province: Tillaberi\n",
      "Error with: GourÃ©\n",
      "Replaced 'GourÃ©' with 'Goure', found province: Zinder\n",
      "Error with: MangalmÃ©\n",
      "Replaced 'MangalmÃ©' with 'Mangalme', found province: Guera\n",
      "Error with: IllÃ©la\n",
      "Replaced 'IllÃ©la' with 'Illela', found province: Sokoto\n",
      "Error with: BankilarÃ©\n",
      "Replaced 'BankilarÃ©' with 'Bankilare', found province: Tillaberi\n",
      "Error with: MaÃ¯nÃ© Soroa\n",
      "Modified name 'MaÃ¯ne Soroa' not in districts.\n",
      "Error with: GothÃ¨ye\n",
      "Replaced 'GothÃ¨ye' with 'Gotheye', found province: Tillaberi\n",
      "Error with: La PendÃ©\n",
      "Replaced 'La PendÃ©' with 'La Pende', found province: Logone Oriental\n"
     ]
    }
   ],
   "source": [
    "admin_to_province = {}\n",
    "for a in admin_names:\n",
    "    try:\n",
    "        admin_to_province[a] = find_province(a)\n",
    "    except Exception as e:\n",
    "        # Print the admin name that caused an error\n",
    "        print(\"Error with:\", a)\n",
    "        # Check if a contains accented characters \"Ã©\" or \"Ã¨\"\n",
    "        if 'Ã©' in a or 'Ã¨' in a or 'Ã´' in a:\n",
    "            a_modified = a.replace('Ã©', 'e').replace('Ã¨', 'e').replace('Ã´', 'o')\n",
    "            # Check if the modified name is in districts\n",
    "            if a_modified in districts:\n",
    "                # Use the modified name to look up the province from admins\n",
    "                try:\n",
    "                    province = admins.filter(pl.col('district') == a_modified)['province'][0]\n",
    "                    admin_to_province[a] = province\n",
    "                    print(f\"Replaced '{a}' with '{a_modified}', found province: {province}\")\n",
    "                except Exception as ex:\n",
    "                    print(f\"Modified name '{a_modified}' not found in admins: {ex}\")\n",
    "            else:\n",
    "                print(f\"Modified name '{a_modified}' not in districts.\")\n",
    "        else:\n",
    "            print(f\"No accented e found in '{a}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Administrative Names to Provinces in time_series\n",
    "\n",
    "Maps `admin_name` to their respective **provinces** using a precomputed dictionary - >`admin_to_province` in `time_series`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = time_series.with_columns(\n",
    "    pl.col('admin_name').map_elements(\n",
    "        lambda x: admin_to_province[x] if x in admin_to_province else admin_to_province.get(x.replace('Ã´', 'o')),\n",
    "        return_dtype=pl.Utf8\n",
    "    ).alias('province')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â³ Time Lagging & Feature Engineering\n",
    "\n",
    "#### ðŸ“… **Why Use Lagging?**\n",
    "\n",
    "To predict food insecurity **for a given quarter**, we use:\n",
    "\n",
    "- **6 months of historical values** for traditional & news-based features.\n",
    "- **Province & country-level aggregations** to capture broader shocks.\n",
    "- **6 quarters of lagged IPC phase values** to model temporal dependencies.\n",
    "\n",
    "#### âš¡ **Optimized Lagging Approach**\n",
    "\n",
    "To improve computational efficiency, we:\n",
    "âœ” Use `groupby()` for **fast province & country-level aggregations**.  \n",
    "âœ” Merge lagged data via `merge()` instead of slow `.apply()`.  \n",
    "âœ” Only keep **past data** to ensure no data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_lagged(features, start=3, end=9, diff=1, agg=True, time_series=time_series):\n",
    "    levels = ['', '_province', '_country'] if agg else ['']\n",
    "    \n",
    "    # Work on a copy to avoid modifying the original during processing\n",
    "    working_df = time_series.clone()\n",
    "    \n",
    "    # Precompute a mapping for each feature (with its suffix) for fast lookups.\n",
    "    # For each row, its lookup key will be: admin_code + '_' + year_month.\n",
    "    lookup_maps = {}  # dict mapping f_s -> mapping dict\n",
    "    for suffix in levels:\n",
    "        for f in features:\n",
    "            f_s = f + suffix\n",
    "            \n",
    "            # Create a DataFrame with just the columns we need and create the key\n",
    "            keys_df = working_df.select(\n",
    "                (pl.col(\"admin_code\").cast(str) + \"_\" + pl.col(\"year_month\").cast(str)).alias(\"key\"),\n",
    "                pl.col(f_s)\n",
    "            )\n",
    "            \n",
    "            # Group by key and take first value (handles duplicates efficiently)\n",
    "            # Then convert directly to dict\n",
    "            mapping = keys_df.group_by(\"key\").agg(pl.col(f_s).first()).to_dict(as_series=False)\n",
    "            mapping = dict(zip(mapping[\"key\"], mapping[f_s]))\n",
    "            \n",
    "            lookup_maps[f_s] = mapping\n",
    "\n",
    "    # Prepare list to collect all new columns (as Series)\n",
    "    new_cols = {}\n",
    "    \n",
    "    # Process each feature and lag combination\n",
    "    for suffix in levels:\n",
    "        for f in features:\n",
    "            f_s = f + suffix\n",
    "            mapping = lookup_maps[f_s]\n",
    "            for t in range(start, end, diff):\n",
    "                col_name = f\"{f_s}_{t}\"\n",
    "                if col_name in time_series.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Create DataFrame with lagged months and years\n",
    "                temp_df = working_df.with_columns([\n",
    "                    pl.when(pl.col(\"month\") - t <= 0)\n",
    "                    .then(pl.col(\"year\") - 1)\n",
    "                    .otherwise(pl.col(\"year\"))\n",
    "                    .alias(\"l_year\"),\n",
    "                    \n",
    "                    ((pl.col(\"month\") - 1 - t) % 12 + 1).alias(\"l_month\")\n",
    "                ])\n",
    "                \n",
    "                # Create reference key\n",
    "                temp_df = temp_df.with_columns(\n",
    "                    (pl.col(\"admin_code\").cast(str) + \"_\" + \n",
    "                    pl.col(\"l_year\").cast(str) + \"_\" + \n",
    "                    pl.col(\"l_month\").cast(str)).alias(\"ref_key\")\n",
    "                )\n",
    "                \n",
    "                # Do efficient lookup using the precomputed mapping\n",
    "                keys = list(mapping.keys())\n",
    "                values = [mapping[k] for k in keys]\n",
    "                lookup_df = pl.DataFrame({\"key\": keys, \"value\": values})\n",
    "                \n",
    "                # Join with lookup DataFrame to get mapped values\n",
    "                result = temp_df.join(\n",
    "                    lookup_df, \n",
    "                    left_on=\"ref_key\", \n",
    "                    right_on=\"key\", \n",
    "                    how=\"left\"\n",
    "                )\n",
    "                \n",
    "                # Fill null values with original values from f_s\n",
    "                result = result.with_columns(\n",
    "                    pl.when(pl.col(\"value\").is_null())\n",
    "                    .then(pl.col(f_s))\n",
    "                    .otherwise(pl.col(\"value\"))\n",
    "                    .alias(col_name)\n",
    "                )\n",
    "                \n",
    "                # Add to new_cols\n",
    "                new_cols[col_name] = result.select(pl.col(col_name)).to_series()\n",
    "                \n",
    "    # If any new columns were created, add them to the original time_series DataFrame.\n",
    "    if new_cols:\n",
    "        new_cols_df = pl.DataFrame(new_cols)\n",
    "        time_series = time_series.hstack(new_cols_df)\n",
    "        \n",
    "        \n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Province & Country-Level Aggregation\n",
    "\n",
    "This function aggregates feature values at the province and country levels to capture regional trends, aiding in food insecurity prediction. The process includes:\n",
    "\n",
    "- **Grouping by year_month and level:** Data is grouped by year_month and the specified level (province or country) to calculate the mean of features, reflecting regional trends over time.\n",
    "\n",
    "- **Applying transformations efficiently:** Instead of merging aggregated data, `transform(\"mean\")` is used to directly assign the computed mean to each row, avoiding unnecessary joins and improving performance.  \n",
    "\n",
    "#### âš¡ **Efficiency Gains**\n",
    "\n",
    "- **Fast Aggregation**: Uses `groupby()` for efficient aggregation.\n",
    "- **Avoids Costly Joins**: Eliminates the need for `merge()` by using `transform()` instead, reducing computational overhead.  \n",
    "- **Memory Efficiency**: Converts the `level` column to a categorical type to reduce memory usage.\n",
    "\n",
    "This approach ensures faster processing while maintaining the quality of aggregated features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_agg_factors(features, level):\n",
    "    global time_series\n",
    "      \n",
    "    # Create expressions for each feature to calculate the mean and rename\n",
    "    mean_exprs = [\n",
    "        pl.col(f).mean().over([\"year_month\", level]).alias(f\"{f}_{level}\")\n",
    "        for f in features\n",
    "    ]\n",
    "    \n",
    "    # Add the new aggregated columns to the DataFrame\n",
    "    time_series = time_series.with_columns(mean_exprs)\n",
    "    \n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 895)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>index</th><th>country</th><th>admin_code</th><th>admin_name</th><th>centx</th><th>centy</th><th>year_month</th><th>year</th><th>month</th><th>fews_ipc</th><th>fews_ha</th><th>fews_proj_near</th><th>fews_proj_near_ha</th><th>fews_proj_med</th><th>fews_proj_med_ha</th><th>ndvi_mean</th><th>ndvi_anom</th><th>rain_mean</th><th>rain_anom</th><th>et_mean</th><th>et_anom</th><th>acled_count</th><th>acled_fatalities</th><th>p_staple_food</th><th>area</th><th>cropland_pct</th><th>pop</th><th>ruggedness_mean</th><th>pasture_pct</th><th>change_fews</th><th>land seizures_0</th><th>land seizures_1</th><th>land seizures_2</th><th>slashed export_0</th><th>slashed export_1</th><th>slashed export_2</th><th>&hellip;</th><th>terrorist_0_province</th><th>warlord_0_province</th><th>d&#x27;etat_0_province</th><th>overthrow_0_province</th><th>convoys_0_province</th><th>carbon_0_province</th><th>mayhem_0_province</th><th>dehydrated_0_province</th><th>mismanagement_0_province</th><th>ndvi_mean_province</th><th>ndvi_anom_province</th><th>rain_mean_province</th><th>rain_anom_province</th><th>et_mean_province</th><th>et_anom_province</th><th>acled_count_province</th><th>acled_fatalities_province</th><th>p_staple_food_province</th><th>ndvi_mean_country</th><th>ndvi_anom_country</th><th>rain_mean_country</th><th>rain_anom_country</th><th>et_mean_country</th><th>et_anom_country</th><th>acled_count_country</th><th>acled_fatalities_country</th><th>p_staple_food_country</th><th>area_province</th><th>cropland_pct_province</th><th>pop_province</th><th>ruggedness_mean_province</th><th>pasture_pct_province</th><th>area_country</th><th>cropland_pct_country</th><th>pop_country</th><th>ruggedness_mean_country</th><th>pasture_pct_country</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>11</td><td>63</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2012_04&quot;</td><td>2012</td><td>4</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.116692</td><td>98.511837</td><td>2.908423</td><td>0.510019</td><td>4.698466</td><td>2.075454</td><td>0</td><td>0</td><td>1.019085</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.864333</td><td>0.262</td><td>1.209</td><td>0.010333</td><td>-0.543667</td><td>-0.852667</td><td>&hellip;</td><td>-0.224</td><td>0.404</td><td>0.706</td><td>-0.343667</td><td>-0.690333</td><td>-0.267</td><td>-0.291333</td><td>-0.553333</td><td>1.016667</td><td>0.116692</td><td>98.511837</td><td>2.908423</td><td>0.510019</td><td>4.698466</td><td>2.075454</td><td>0.0</td><td>0.0</td><td>1.019085</td><td>0.168389</td><td>100.05702</td><td>8.605167</td><td>1.043137</td><td>8.770066</td><td>2.15922</td><td>0.0</td><td>0.0</td><td>1.151653</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>18883.616188</td><td>7.335028</td><td>874755.441176</td><td>316314.116188</td><td>49.145729</td></tr><tr><td>12</td><td>66</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2012_07&quot;</td><td>2012</td><td>7</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.104068</td><td>104.571165</td><td>0.363717</td><td>-0.060719</td><td>0.254137</td><td>-0.010891</td><td>0</td><td>0</td><td>1.031224</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.582333</td><td>0.015333</td><td>1.219667</td><td>-0.936667</td><td>-0.993667</td><td>1.612</td><td>&hellip;</td><td>0.169333</td><td>0.063667</td><td>-0.807333</td><td>-0.633667</td><td>-0.818667</td><td>0.846667</td><td>1.122</td><td>0.224</td><td>0.184</td><td>0.104068</td><td>104.571165</td><td>0.363717</td><td>-0.060719</td><td>0.254137</td><td>-0.010891</td><td>0.0</td><td>0.0</td><td>1.031224</td><td>0.172991</td><td>107.329397</td><td>1.145694</td><td>-0.271082</td><td>2.026944</td><td>0.019929</td><td>0.0</td><td>0.0</td><td>1.172457</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>18883.616188</td><td>7.335028</td><td>874755.441176</td><td>316314.116188</td><td>49.145729</td></tr><tr><td>13</td><td>69</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2012_10&quot;</td><td>2012</td><td>10</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.101172</td><td>104.14274</td><td>0.452095</td><td>-0.073343</td><td>0.628785</td><td>0.159913</td><td>0</td><td>0</td><td>1.194955</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.964333</td><td>-0.597333</td><td>-0.034333</td><td>-0.6</td><td>0.105</td><td>-0.277333</td><td>&hellip;</td><td>1.068</td><td>-0.714333</td><td>1.443</td><td>1.471333</td><td>0.113</td><td>1.133333</td><td>0.344667</td><td>-0.665667</td><td>-0.436667</td><td>0.101172</td><td>104.14274</td><td>0.452095</td><td>-0.073343</td><td>0.628785</td><td>0.159913</td><td>0.0</td><td>0.0</td><td>1.194955</td><td>0.139471</td><td>104.454137</td><td>1.890517</td><td>0.441801</td><td>2.518038</td><td>0.56151</td><td>0.0</td><td>0.0</td><td>1.300843</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>18883.616188</td><td>7.335028</td><td>874755.441176</td><td>316314.116188</td><td>49.145729</td></tr><tr><td>14</td><td>72</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2013_01&quot;</td><td>2013</td><td>1</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.095679</td><td>97.278972</td><td>2.836539</td><td>-3.39087</td><td>4.793217</td><td>-0.099054</td><td>0</td><td>0</td><td>1.157092</td><td>54174.53381</td><td>1.417796</td><td>1.429508e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.426</td><td>-0.806</td><td>0.714</td><td>-0.188667</td><td>0.121667</td><td>1.336</td><td>&hellip;</td><td>-0.805333</td><td>0.564333</td><td>1.416</td><td>1.163</td><td>-0.271</td><td>1.477</td><td>-0.470333</td><td>-0.641667</td><td>0.95</td><td>0.095679</td><td>97.278972</td><td>2.836539</td><td>-3.39087</td><td>4.793217</td><td>-0.099054</td><td>0.0</td><td>0.0</td><td>1.157092</td><td>0.074545</td><td>90.765142</td><td>3.960445</td><td>-2.072366</td><td>5.509975</td><td>0.162572</td><td>0.0</td><td>0.0</td><td>1.368094</td><td>54174.53381</td><td>1.417796</td><td>1.429508e6</td><td>101047.1587</td><td>16.246279</td><td>18883.616188</td><td>7.335028</td><td>901519.941176</td><td>316314.116188</td><td>49.145729</td></tr><tr><td>15</td><td>75</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2013_04&quot;</td><td>2013</td><td>4</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.135269</td><td>114.19412</td><td>2.997978</td><td>0.599575</td><td>3.522834</td><td>0.899823</td><td>0</td><td>0</td><td>1.042512</td><td>54174.53381</td><td>1.417796</td><td>1.429508e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>-0.503</td><td>-0.131333</td><td>1.169333</td><td>-0.295667</td><td>1.138</td><td>0.287</td><td>&hellip;</td><td>1.436333</td><td>0.916</td><td>0.927</td><td>0.314333</td><td>-0.043667</td><td>1.304333</td><td>0.085333</td><td>1.046333</td><td>0.838667</td><td>0.135269</td><td>114.19412</td><td>2.997978</td><td>0.599575</td><td>3.522834</td><td>0.899823</td><td>0.0</td><td>0.0</td><td>1.042512</td><td>0.181455</td><td>106.985048</td><td>10.050844</td><td>2.488815</td><td>8.968665</td><td>2.35782</td><td>0.0</td><td>0.0</td><td>1.414363</td><td>54174.53381</td><td>1.417796</td><td>1.429508e6</td><td>101047.1587</td><td>16.246279</td><td>18883.616188</td><td>7.335028</td><td>901519.941176</td><td>316314.116188</td><td>49.145729</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 895)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚     â”† index â”† country     â”† admin_code â”† â€¦ â”† cropland_pc â”† pop_country â”† ruggedness â”† pasture_pc â”‚\n",
       "â”‚ --- â”† ---   â”† ---         â”† ---        â”†   â”† t_country   â”† ---         â”† _mean_coun â”† t_country  â”‚\n",
       "â”‚ i64 â”† i64   â”† str         â”† i64        â”†   â”† ---         â”† f64         â”† try        â”† ---        â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”† f64         â”†             â”† ---        â”† f64        â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”†             â”† f64        â”†            â”‚\n",
       "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 11  â”† 63    â”† Afghanistan â”† 202        â”† â€¦ â”† 7.335028    â”† 874755.4411 â”† 316314.116 â”† 49.145729  â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”† 76          â”† 188        â”†            â”‚\n",
       "â”‚ 12  â”† 66    â”† Afghanistan â”† 202        â”† â€¦ â”† 7.335028    â”† 874755.4411 â”† 316314.116 â”† 49.145729  â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”† 76          â”† 188        â”†            â”‚\n",
       "â”‚ 13  â”† 69    â”† Afghanistan â”† 202        â”† â€¦ â”† 7.335028    â”† 874755.4411 â”† 316314.116 â”† 49.145729  â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”† 76          â”† 188        â”†            â”‚\n",
       "â”‚ 14  â”† 72    â”† Afghanistan â”† 202        â”† â€¦ â”† 7.335028    â”† 901519.9411 â”† 316314.116 â”† 49.145729  â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”† 76          â”† 188        â”†            â”‚\n",
       "â”‚ 15  â”† 75    â”† Afghanistan â”† 202        â”† â€¦ â”† 7.335028    â”† 901519.9411 â”† 316314.116 â”† 49.145729  â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”† 76          â”† 188        â”†            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregating news factors\n",
    "time_series = add_agg_factors(news_factors, level='country')\n",
    "time_series = add_agg_factors(news_factors, level='province')\n",
    "\n",
    "# Aggregating variant traditional factors\n",
    "time_series = add_agg_factors(t_variant_traditional_factors, level='province')\n",
    "time_series = add_agg_factors(t_variant_traditional_factors, level='country')\n",
    "\n",
    "# Aggregating invariant traditional factors\n",
    "time_series = add_agg_factors(t_invariant_traditional_factors, level='province')\n",
    "time_series = add_agg_factors(t_invariant_traditional_factors, level='country')\n",
    "\n",
    "# Drop null values\n",
    "time_series = time_series.drop_nulls()\n",
    "time_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add time lagged features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28141, 4070)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = add_time_lagged(t_variant_traditional_factors, time_series=time_series)\n",
    "time_series = add_time_lagged(news_factors, time_series=time_series)\n",
    "time_series = add_time_lagged(['fews_ipc'], end=21, diff=3, agg=False, time_series=time_series)\n",
    "time_series = add_time_lagged(['fews_proj_near'], start=3, end=4, diff=1, agg=False, time_series=time_series)\n",
    "\n",
    "# Drop null values again\n",
    "time_series = time_series.drop_nulls()\n",
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4_070)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>index</th><th>country</th><th>admin_code</th><th>admin_name</th><th>centx</th><th>centy</th><th>year_month</th><th>year</th><th>month</th><th>fews_ipc</th><th>fews_ha</th><th>fews_proj_near</th><th>fews_proj_near_ha</th><th>fews_proj_med</th><th>fews_proj_med_ha</th><th>ndvi_mean</th><th>ndvi_anom</th><th>rain_mean</th><th>rain_anom</th><th>et_mean</th><th>et_anom</th><th>acled_count</th><th>acled_fatalities</th><th>p_staple_food</th><th>area</th><th>cropland_pct</th><th>pop</th><th>ruggedness_mean</th><th>pasture_pct</th><th>change_fews</th><th>land seizures_0</th><th>land seizures_1</th><th>land seizures_2</th><th>slashed export_0</th><th>slashed export_1</th><th>slashed export_2</th><th>&hellip;</th><th>convoys_0_country_3</th><th>convoys_0_country_4</th><th>convoys_0_country_5</th><th>convoys_0_country_6</th><th>convoys_0_country_7</th><th>convoys_0_country_8</th><th>carbon_0_country_3</th><th>carbon_0_country_4</th><th>carbon_0_country_5</th><th>carbon_0_country_6</th><th>carbon_0_country_7</th><th>carbon_0_country_8</th><th>mayhem_0_country_3</th><th>mayhem_0_country_4</th><th>mayhem_0_country_5</th><th>mayhem_0_country_6</th><th>mayhem_0_country_7</th><th>mayhem_0_country_8</th><th>dehydrated_0_country_3</th><th>dehydrated_0_country_4</th><th>dehydrated_0_country_5</th><th>dehydrated_0_country_6</th><th>dehydrated_0_country_7</th><th>dehydrated_0_country_8</th><th>mismanagement_0_country_3</th><th>mismanagement_0_country_4</th><th>mismanagement_0_country_5</th><th>mismanagement_0_country_6</th><th>mismanagement_0_country_7</th><th>mismanagement_0_country_8</th><th>fews_ipc_3</th><th>fews_ipc_6</th><th>fews_ipc_9</th><th>fews_ipc_12</th><th>fews_ipc_15</th><th>fews_ipc_18</th><th>fews_proj_near_3</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>11</td><td>63</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2012_04&quot;</td><td>2012</td><td>4</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.116692</td><td>98.511837</td><td>2.908423</td><td>0.510019</td><td>4.698466</td><td>2.075454</td><td>0</td><td>0</td><td>1.019085</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.864333</td><td>0.262</td><td>1.209</td><td>0.010333</td><td>-0.543667</td><td>-0.852667</td><td>&hellip;</td><td>0.323402</td><td>0.323402</td><td>0.323402</td><td>0.323402</td><td>0.323402</td><td>0.323402</td><td>0.458894</td><td>0.458894</td><td>0.458894</td><td>0.458894</td><td>0.458894</td><td>0.458894</td><td>0.241553</td><td>0.241553</td><td>0.241553</td><td>0.241553</td><td>0.241553</td><td>0.241553</td><td>0.743474</td><td>0.743474</td><td>0.743474</td><td>0.743474</td><td>0.743474</td><td>0.743474</td><td>0.450139</td><td>0.450139</td><td>0.450139</td><td>0.450139</td><td>0.450139</td><td>0.450139</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>12</td><td>66</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2012_07&quot;</td><td>2012</td><td>7</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.104068</td><td>104.571165</td><td>0.363717</td><td>-0.060719</td><td>0.254137</td><td>-0.010891</td><td>0</td><td>0</td><td>1.031224</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.582333</td><td>0.015333</td><td>1.219667</td><td>-0.936667</td><td>-0.993667</td><td>1.612</td><td>&hellip;</td><td>0.207569</td><td>0.207569</td><td>0.207569</td><td>0.207569</td><td>0.207569</td><td>0.207569</td><td>0.417088</td><td>0.417088</td><td>0.417088</td><td>0.417088</td><td>0.417088</td><td>0.417088</td><td>0.30201</td><td>0.30201</td><td>0.30201</td><td>0.30201</td><td>0.30201</td><td>0.30201</td><td>0.219176</td><td>0.219176</td><td>0.219176</td><td>0.219176</td><td>0.219176</td><td>0.219176</td><td>0.398922</td><td>0.398922</td><td>0.398922</td><td>0.398922</td><td>0.398922</td><td>0.398922</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>13</td><td>69</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2012_10&quot;</td><td>2012</td><td>10</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.101172</td><td>104.14274</td><td>0.452095</td><td>-0.073343</td><td>0.628785</td><td>0.159913</td><td>0</td><td>0</td><td>1.194955</td><td>54174.53381</td><td>1.417796</td><td>1.379956e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.964333</td><td>-0.597333</td><td>-0.034333</td><td>-0.6</td><td>0.105</td><td>-0.277333</td><td>&hellip;</td><td>0.588971</td><td>0.588971</td><td>0.588971</td><td>0.588971</td><td>0.588971</td><td>0.588971</td><td>0.160451</td><td>0.160451</td><td>0.160451</td><td>0.160451</td><td>0.160451</td><td>0.160451</td><td>0.421833</td><td>0.421833</td><td>0.421833</td><td>0.421833</td><td>0.421833</td><td>0.421833</td><td>0.373696</td><td>0.373696</td><td>0.373696</td><td>0.373696</td><td>0.373696</td><td>0.373696</td><td>0.2515</td><td>0.2515</td><td>0.2515</td><td>0.2515</td><td>0.2515</td><td>0.2515</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>14</td><td>72</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2013_01&quot;</td><td>2013</td><td>1</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.095679</td><td>97.278972</td><td>2.836539</td><td>-3.39087</td><td>4.793217</td><td>-0.099054</td><td>0</td><td>0</td><td>1.157092</td><td>54174.53381</td><td>1.417796</td><td>1.429508e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>0.426</td><td>-0.806</td><td>0.714</td><td>-0.188667</td><td>0.121667</td><td>1.336</td><td>&hellip;</td><td>0.588971</td><td>0.310735</td><td>0.310735</td><td>0.310735</td><td>0.310735</td><td>0.310735</td><td>0.160451</td><td>0.572353</td><td>0.572353</td><td>0.572353</td><td>0.572353</td><td>0.572353</td><td>0.421833</td><td>0.465657</td><td>0.465657</td><td>0.465657</td><td>0.465657</td><td>0.465657</td><td>0.373696</td><td>0.654549</td><td>0.654549</td><td>0.654549</td><td>0.654549</td><td>0.654549</td><td>0.2515</td><td>0.362775</td><td>0.362775</td><td>0.362775</td><td>0.362775</td><td>0.362775</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>15</td><td>75</td><td>&quot;Afghanistan&quot;</td><td>202</td><td>&quot;Kandahar&quot;</td><td>65.709343</td><td>31.043618</td><td>&quot;2013_04&quot;</td><td>2013</td><td>4</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.135269</td><td>114.19412</td><td>2.997978</td><td>0.599575</td><td>3.522834</td><td>0.899823</td><td>0</td><td>0</td><td>1.042512</td><td>54174.53381</td><td>1.417796</td><td>1.429508e6</td><td>101047.1587</td><td>16.246279</td><td>0.0</td><td>-0.503</td><td>-0.131333</td><td>1.169333</td><td>-0.295667</td><td>1.138</td><td>0.287</td><td>&hellip;</td><td>0.294579</td><td>0.294579</td><td>0.294579</td><td>0.588971</td><td>0.294579</td><td>0.294579</td><td>0.079251</td><td>0.079251</td><td>0.079251</td><td>0.160451</td><td>0.079251</td><td>0.079251</td><td>0.079346</td><td>0.079346</td><td>0.079346</td><td>0.421833</td><td>0.079346</td><td>0.079346</td><td>0.283934</td><td>0.283934</td><td>0.283934</td><td>0.373696</td><td>0.283934</td><td>0.283934</td><td>0.151222</td><td>0.151222</td><td>0.151222</td><td>0.2515</td><td>0.151222</td><td>0.151222</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4_070)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚     â”† index â”† country     â”† admin_code â”† â€¦ â”† fews_ipc_12 â”† fews_ipc_15 â”† fews_ipc_1 â”† fews_proj_ â”‚\n",
       "â”‚ --- â”† ---   â”† ---         â”† ---        â”†   â”† ---         â”† ---         â”† 8          â”† near_3     â”‚\n",
       "â”‚ i64 â”† i64   â”† str         â”† i64        â”†   â”† f64         â”† f64         â”† ---        â”† ---        â”‚\n",
       "â”‚     â”†       â”†             â”†            â”†   â”†             â”†             â”† f64        â”† f64        â”‚\n",
       "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 11  â”† 63    â”† Afghanistan â”† 202        â”† â€¦ â”† 1.0         â”† 1.0         â”† 1.0        â”† 1.0        â”‚\n",
       "â”‚ 12  â”† 66    â”† Afghanistan â”† 202        â”† â€¦ â”† 1.0         â”† 1.0         â”† 1.0        â”† 1.0        â”‚\n",
       "â”‚ 13  â”† 69    â”† Afghanistan â”† 202        â”† â€¦ â”† 1.0         â”† 1.0         â”† 1.0        â”† 1.0        â”‚\n",
       "â”‚ 14  â”† 72    â”† Afghanistan â”† 202        â”† â€¦ â”† 1.0         â”† 1.0         â”† 1.0        â”† 1.0        â”‚\n",
       "â”‚ 15  â”† 75    â”† Afghanistan â”† 202        â”† â€¦ â”† 1.0         â”† 1.0         â”† 1.0        â”† 1.0        â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time with Polars: 22.56 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total execution time with Polars: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from cuml.ensemble import RandomForestRegressor as cuRF\n",
    "from cuml.ensemble import RandomForestClassifier as cuRFC\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_splits = [\n",
    "    ((2012,7), (2013,7)), \n",
    "    ((2013,7), (2014,7)), \n",
    "    ((2014,7), (2015,7)), \n",
    "    ((2016,7), (2017,7)), \n",
    "]\n",
    "\n",
    "train_splits = [ \n",
    "    ((2011,7), (2013,7)),\n",
    "    ((2012,7), (2014,1)),\n",
    "    ((2013,7), (2015,10)),\n",
    "    ((2015,7), (2017,2)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn implementation of Random Forest\n",
    "\n",
    "In this part, we will implement the **Random Forest** model using the `scikit-learn` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RF': RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "def get_agg_lagged_features(factors):\n",
    "    return [f\"{f}_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_province_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_country_{t}\" for f in factors for t in range(3, 9)]\n",
    "\n",
    "features = {\n",
    "    'traditional': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors\n",
    "    ),\n",
    "    \n",
    "    'news': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(news_factors)\n",
    "    ),\n",
    "    \n",
    "    'traditional+news': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors + \n",
    "        get_agg_lagged_features(news_factors)\n",
    "    ),   \n",
    "}\n",
    "\n",
    "labels_df = time_series.select(['fews_ipc', 'year', 'month'])\n",
    "\n",
    "def get_time_split(df, start, end):\n",
    "    return df.filter(\n",
    "        ((pl.col('year') > start[0]) | ((pl.col('year') == start[0]) & (pl.col('month') >= start[1]))) &\n",
    "        ((pl.col('year') < end[0]) | ((pl.col('year') == end[0]) & (pl.col('month') <= end[1])))\n",
    "    )\n",
    "\n",
    "thresholds = {\n",
    "    'traditional': (2.236, 3.125), \n",
    "    'news': (1.907, 2.712), \n",
    "    'traditional+news': (2.105, 3.314),\n",
    "}\n",
    "\n",
    "def train_and_evaluate(train, test, f, D):\n",
    "    results = []\n",
    "    \n",
    "    # Get train data using Polars and convert to numpy\n",
    "    train_df = get_time_split(D, train[0], train[1])\n",
    "    X_train = train_df.drop(['year', 'month']).to_numpy()\n",
    "    \n",
    "    train_labels = get_time_split(labels_df, train[0], train[1])\n",
    "    y_train = train_labels.drop(['year', 'month']).to_numpy().ravel()\n",
    "\n",
    "    # Handle NaN values\n",
    "    nan_mask = ~np.isnan(X_train).any(axis=1)\n",
    "    X_train = X_train[nan_mask]\n",
    "    y_train = y_train[nan_mask]\n",
    "    \n",
    "    # Get test data\n",
    "    test_df = get_time_split(D, test[0], test[1])\n",
    "    X_test = test_df.drop(['year', 'month']).to_numpy()\n",
    "    \n",
    "    test_labels = get_time_split(labels_df, test[0], test[1])\n",
    "    y_test = test_labels.drop(['year', 'month']).to_numpy().ravel()\n",
    "    \n",
    "    # Handle NaN values in test data\n",
    "    nan_mask_test = ~np.isnan(X_test).any(axis=1)\n",
    "    X_test = X_test[nan_mask_test]\n",
    "    y_test = y_test[nan_mask_test]\n",
    "    \n",
    "    print(f\"Rows in X_train: {X_train.shape[0]} \\nRows in X_test: {X_test.shape[0]}\")\n",
    "\n",
    "    if X_train.shape[0] <= 0:\n",
    "        return results\n",
    "    if X_test.shape[0] <= 0:\n",
    "        return results\n",
    "\n",
    "    # Get threshold values\n",
    "    lower, upper = thresholds[f]\n",
    "    y_test_binary = np.where((y_test >= lower) & (y_test <= upper), 1, 0)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        \n",
    "        stderr = np.std(y_test - preds) / (np.sqrt(len(y_test)) + 0.0001)\n",
    "        upper_bound = np.sqrt(rmse**2 + 1.96 * stderr)\n",
    "        lower_bound = np.sqrt(rmse**2 - 1.96 * stderr)\n",
    "\n",
    "        results.append({\n",
    "            'method': name, \n",
    "            'split': test, \n",
    "            'features': f, \n",
    "            'rmse': rmse,  \n",
    "            'lower_bound': lower_bound, \n",
    "            'upper_bound': upper_bound,\n",
    "        })\n",
    "\n",
    "        print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f} [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run model evaluation\n",
    "all_results = []\n",
    "for train, test in zip(train_splits, test_splits):\n",
    "    for f, D in features.items():\n",
    "        try:\n",
    "            result = train_and_evaluate(train, test, f, D)\n",
    "            all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} on {train} & {test}\")\n",
    "            continue\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_rfr_sklearn = pl.DataFrame([res for sublist in all_results for res in sublist])\n",
    "# fig_3a.write_csv('fig_3a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>method</th><th>features</th><th>mean_rmse</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;RF&quot;</td><td>&quot;news&quot;</td><td>0.017914</td></tr><tr><td>&quot;RF&quot;</td><td>&quot;traditional+news&quot;</td><td>0.019351</td></tr><tr><td>&quot;RF&quot;</td><td>&quot;traditional&quot;</td><td>0.027491</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ method â”† features         â”† mean_rmse â”‚\n",
       "â”‚ ---    â”† ---              â”† ---       â”‚\n",
       "â”‚ str    â”† str              â”† f64       â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ RF     â”† news             â”† 0.017914  â”‚\n",
       "â”‚ RF     â”† traditional+news â”† 0.019351  â”‚\n",
       "â”‚ RF     â”† traditional      â”† 0.027491  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_sklearn  = results_rfr_sklearn.group_by(['method', 'features']).agg(\n",
    "    pl.mean('rmse').alias('mean_rmse'),\n",
    ")\n",
    "\n",
    "agg_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CuML implementation of Random Forest\n",
    "\n",
    "In this part, we will implement the **Random Forest** model using the `cuml` library. This is a GPU-accelerated library that provides a similar API to `scikit-learn`, allowing us to leverage the power of GPUs for faster computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in X_train: 6110 \n",
      "Rows in X_test: 5127\n",
      "Method: RF, Split: ((2012, 7), (2013, 7)), Features: traditional, RMSE: 0.0004 [nan, 0.0033]\n",
      "Rows in X_train: 6110 \n",
      "Rows in X_test: 5127\n",
      "Method: RF, Split: ((2012, 7), (2013, 7)), Features: news, RMSE: 0.0020 [nan, 0.0077]\n",
      "Rows in X_train: 6110 \n",
      "Rows in X_test: 5127\n",
      "Method: RF, Split: ((2012, 7), (2013, 7)), Features: traditional+news, RMSE: 0.0020 [nan, 0.0077]\n",
      "Rows in X_train: 7131 \n",
      "Rows in X_test: 4976\n",
      "Method: RF, Split: ((2013, 7), (2014, 7)), Features: traditional, RMSE: 0.0899 [0.0748, 0.1028]\n",
      "Rows in X_train: 7131 \n",
      "Rows in X_test: 4976\n",
      "Method: RF, Split: ((2013, 7), (2014, 7)), Features: news, RMSE: 0.0635 [0.0476, 0.0761]\n",
      "Rows in X_train: 7131 \n",
      "Rows in X_test: 4976\n",
      "Method: RF, Split: ((2013, 7), (2014, 7)), Features: traditional+news, RMSE: 0.0636 [0.0478, 0.0762]\n",
      "Rows in X_train: 10361 \n",
      "Rows in X_test: 5485\n",
      "Method: RF, Split: ((2014, 7), (2015, 7)), Features: traditional, RMSE: 0.0061 [nan, 0.0141]\n",
      "Rows in X_train: 10361 \n",
      "Rows in X_test: 5485\n",
      "Method: RF, Split: ((2014, 7), (2015, 7)), Features: news, RMSE: 0.0117 [nan, 0.0211]\n",
      "Rows in X_train: 10361 \n",
      "Rows in X_test: 5485\n",
      "Method: RF, Split: ((2014, 7), (2015, 7)), Features: traditional+news, RMSE: 0.0117 [nan, 0.0212]\n",
      "Rows in X_train: 6472 \n",
      "Rows in X_test: 3335\n",
      "Method: RF, Split: ((2016, 7), (2017, 7)), Features: traditional, RMSE: 0.0071 [nan, 0.0171]\n",
      "Rows in X_train: 6472 \n",
      "Rows in X_test: 3335\n",
      "Method: RF, Split: ((2016, 7), (2017, 7)), Features: news, RMSE: 0.0071 [nan, 0.0171]\n",
      "Rows in X_train: 6472 \n",
      "Rows in X_test: 3335\n",
      "Method: RF, Split: ((2016, 7), (2017, 7)), Features: traditional+news, RMSE: 0.0071 [nan, 0.0171]\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'RF': cuRF(),\n",
    "}\n",
    "\n",
    "def get_agg_lagged_features(factors):\n",
    "    return [f\"{f}_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_province_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_country_{t}\" for f in factors for t in range(3, 9)]\n",
    "\n",
    "features = {\n",
    "    'traditional': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors\n",
    "    ),\n",
    "    \n",
    "    'news': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(news_factors)\n",
    "    ),\n",
    "    \n",
    "    'traditional+news': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors + \n",
    "        get_agg_lagged_features(news_factors)\n",
    "    ),   \n",
    "}\n",
    "\n",
    "labels_df = time_series.select(['fews_ipc', 'year', 'month'])\n",
    "\n",
    "def get_time_split(df, start, end):\n",
    "    return df.filter(\n",
    "        ((pl.col('year') > start[0]) | ((pl.col('year') == start[0]) & (pl.col('month') >= start[1]))) &\n",
    "        ((pl.col('year') < end[0]) | ((pl.col('year') == end[0]) & (pl.col('month') <= end[1])))\n",
    "    )\n",
    "\n",
    "thresholds = {\n",
    "    'traditional': (2.236, 3.125), \n",
    "    'news': (1.907, 2.712), \n",
    "    'traditional+news': (2.105, 3.314),\n",
    "}\n",
    "\n",
    "def train_and_evaluate(train, test, f, D):\n",
    "    results = []\n",
    "    \n",
    "    # Get train data using Polars and convert to numpy for cuML\n",
    "    train_df = get_time_split(D, train[0], train[1])\n",
    "    X_train = train_df.drop(['year', 'month']).to_numpy()\n",
    "    \n",
    "    train_labels = get_time_split(labels_df, train[0], train[1])\n",
    "    y_train = train_labels.drop(['year', 'month']).to_numpy().ravel()\n",
    "\n",
    "    # Handle NaN values\n",
    "    nan_mask = np.isnan(X_train).any(axis=1)\n",
    "    X_train = X_train[~nan_mask]\n",
    "    y_train = y_train[~nan_mask]\n",
    "    \n",
    "    # Get test data using Polars and convert to numpy for cuML\n",
    "    test_df = get_time_split(D, test[0], test[1])\n",
    "    X_test = test_df.drop(['year', 'month']).to_numpy()\n",
    "    \n",
    "    test_labels = get_time_split(labels_df, test[0], test[1])\n",
    "    y_test = test_labels.drop(['year', 'month']).to_numpy().ravel()\n",
    "    \n",
    "    # Handle NaN values in test data\n",
    "    nan_mask_test = np.isnan(X_test).any(axis=1)\n",
    "    X_test = X_test[~nan_mask_test]\n",
    "    y_test = y_test[~nan_mask_test]\n",
    "    \n",
    "    print(f\"Rows in X_train: {X_train.shape[0]} \\nRows in X_test: {X_test.shape[0]}\")\n",
    "    # Convert to cupy arrays for GPU processing\n",
    "    X_train = cp.asarray(X_train, dtype=cp.float32)\n",
    "    y_train = cp.asarray(y_train, dtype=cp.float32)\n",
    "    X_test = cp.asarray(X_test, dtype=cp.float32)\n",
    "    y_test = cp.asarray(y_test, dtype=cp.float32)\n",
    "    \n",
    "    # Check if data is available\n",
    "    if X_train.shape[0] <= 0:\n",
    "        return results\n",
    "    if X_test.shape[0] <= 0:\n",
    "        return results\n",
    "       \n",
    "    # Get threshold values\n",
    "    lower, upper = thresholds[f]\n",
    "    y_test_binary = cp.where((y_test >= lower) & (y_test <= upper), 1, 0)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(np.mean((y_test - preds) ** 2)).get()\n",
    "        \n",
    "        stderr = np.std(y_test - preds) / (np.sqrt(len(y_test))+0.0001)\n",
    "        upper_bound = np.sqrt(rmse**2 + 1.96 * stderr)\n",
    "        lower_bound = np.sqrt(rmse**2 - 1.96 * stderr)\n",
    "        # precision, recall, _ = precision_recall_curve(y_test_binary, preds)\n",
    "        # aucpr = auc(recall, precision)\n",
    "        \n",
    "        results.append({\n",
    "            'method': name, \n",
    "            'split': test, \n",
    "            'features': f, \n",
    "            'rmse': rmse,  \n",
    "            'lower_bound': lower_bound, \n",
    "            'upper_bound': upper_bound,\n",
    "            # 'aucpr': aucpr\n",
    "        })\n",
    "\n",
    "        # print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f}\")\n",
    "        print(f\"Method: {name}, Split: {test}, Features: {f}, RMSE: {rmse:.4f} [{lower_bound:.4f}, {upper_bound:.4f}]\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run model evaluation\n",
    "all_results = []\n",
    "for train, test in zip(train_splits, test_splits):\n",
    "    for f, D in features.items():\n",
    "        try:\n",
    "            result = train_and_evaluate(train, test, f, D)\n",
    "            all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} on {train} & {test}\")\n",
    "            continue\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_rfr_cuml = pl.DataFrame([res for sublist in all_results for res in sublist])\n",
    "# To save results\n",
    "# results_rfr_cuml.write_csv('fig_3a.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits and Train/Test Set Sizes\n",
    "\n",
    "### 1. Split: ((2012, 7), (2013, 7))\n",
    "- Rows in Train set: 6110\n",
    "- Rows in Test set: 5127\n",
    "\n",
    "### 2. Split: ((2013, 7), (2014, 7))\n",
    "- Rows in Train set: 7131\n",
    "- Rows in Test set: 4976\n",
    "\n",
    "### 3. Split: ((2014, 7), (2015, 7))\n",
    "- Rows in Train set: 10361\n",
    "- Rows in Test set: 5485\n",
    "\n",
    "### 4. Split: ((2016, 7), (2017, 7))\n",
    "- Rows in Train set: 6472\n",
    "- Rows in Test set: 3335\n",
    "\n",
    "### Average sizes of Train and Test sets\n",
    "- Rows in Train set: 7516\n",
    "- Rows in Test set: 4730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>method</th><th>features</th><th>mean_rmse</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;RF&quot;</td><td>&quot;news&quot;</td><td>0.021064</td></tr><tr><td>&quot;RF&quot;</td><td>&quot;traditional+news&quot;</td><td>0.021106</td></tr><tr><td>&quot;RF&quot;</td><td>&quot;traditional&quot;</td><td>0.025874</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ method â”† features         â”† mean_rmse â”‚\n",
       "â”‚ ---    â”† ---              â”† ---       â”‚\n",
       "â”‚ str    â”† str              â”† f64       â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ RF     â”† news             â”† 0.021064  â”‚\n",
       "â”‚ RF     â”† traditional+news â”† 0.021106  â”‚\n",
       "â”‚ RF     â”† traditional      â”† 0.025874  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_cuml  = results_rfr_cuml.group_by(['method', 'features']).agg(\n",
    "    pl.mean('rmse').alias('mean_rmse')\n",
    ")\n",
    "\n",
    "agg_cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticAT model\n",
    "\n",
    "The `mord.LogisticAT` model is an ordinal logistic regression that respects the natural ordering between IPC phases, learning thresholds between classes instead of treating them as unrelated. It performs exceptionally well on news features because news data often contains sharp, timely signals that align closely with IPC phase shifts. As a result, LogisticAT can accurately separate classes when the input features reflect structured, incremental changes, leading to much higher accuracy compared to traditional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Ordinal_LogisticAT, Split: ((2012, 7), (2013, 7)), Features: traditional, Cross-Entropy Loss: 0.8928, Accuracy: 0.6860\n",
      "Method: Ordinal_LogisticAT, Split: ((2012, 7), (2013, 7)), Features: news, Cross-Entropy Loss: 0.0733, Accuracy: 0.9799\n",
      "Method: Ordinal_LogisticAT, Split: ((2012, 7), (2013, 7)), Features: traditional+news, Cross-Entropy Loss: 0.8806, Accuracy: 0.6813\n",
      "Method: Ordinal_LogisticAT, Split: ((2013, 7), (2014, 7)), Features: traditional, Cross-Entropy Loss: 0.7473, Accuracy: 0.7170\n",
      "Method: Ordinal_LogisticAT, Split: ((2013, 7), (2014, 7)), Features: news, Cross-Entropy Loss: 0.0340, Accuracy: 0.9895\n",
      "Method: Ordinal_LogisticAT, Split: ((2013, 7), (2014, 7)), Features: traditional+news, Cross-Entropy Loss: 0.7139, Accuracy: 0.7297\n",
      "Method: Ordinal_LogisticAT, Split: ((2014, 7), (2015, 7)), Features: traditional, Cross-Entropy Loss: 0.8837, Accuracy: 0.7227\n",
      "Method: Ordinal_LogisticAT, Split: ((2014, 7), (2015, 7)), Features: news, Cross-Entropy Loss: 0.0170, Accuracy: 0.9958\n",
      "Method: Ordinal_LogisticAT, Split: ((2014, 7), (2015, 7)), Features: traditional+news, Cross-Entropy Loss: 0.8161, Accuracy: 0.6950\n",
      "Method: Ordinal_LogisticAT, Split: ((2016, 7), (2017, 7)), Features: traditional, Cross-Entropy Loss: 1.4926, Accuracy: 0.6027\n",
      "Method: Ordinal_LogisticAT, Split: ((2016, 7), (2017, 7)), Features: news, Cross-Entropy Loss: 0.0015, Accuracy: 0.9997\n",
      "Method: Ordinal_LogisticAT, Split: ((2016, 7), (2017, 7)), Features: traditional+news, Cross-Entropy Loss: 1.0769, Accuracy: 0.6300\n",
      "Method: Ordinal_LogisticAT, Split: ((2017, 7), (2018, 7)), Features: traditional, Cross-Entropy Loss: 1.2084, Accuracy: 0.6064\n",
      "Method: Ordinal_LogisticAT, Split: ((2017, 7), (2018, 7)), Features: news, Cross-Entropy Loss: 0.0029, Accuracy: 1.0000\n",
      "Method: Ordinal_LogisticAT, Split: ((2017, 7), (2018, 7)), Features: traditional+news, Cross-Entropy Loss: 1.1568, Accuracy: 0.6051\n",
      "Method: Ordinal_LogisticAT, Split: ((2017, 7), (2018, 6)), Features: traditional, Cross-Entropy Loss: 1.2084, Accuracy: 0.6064\n",
      "Method: Ordinal_LogisticAT, Split: ((2017, 7), (2018, 6)), Features: news, Cross-Entropy Loss: 0.0029, Accuracy: 1.0000\n",
      "Method: Ordinal_LogisticAT, Split: ((2017, 7), (2018, 6)), Features: traditional+news, Cross-Entropy Loss: 1.1568, Accuracy: 0.6051\n"
     ]
    }
   ],
   "source": [
    "import mord\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "test_splits = [\n",
    "    ((2012,7), (2013,7)),  \n",
    "    ((2013,7), (2014,7)), \n",
    "    ((2014,7), (2015,7)),  \n",
    "    ((2016,7), (2017,7)),  \n",
    "    ((2017,7), (2018,7)),  \n",
    "    ((2017,7), (2018,6)),  \n",
    "]\n",
    "\n",
    "train_splits = [\n",
    "    ((2011,7), (2012,6)),  \n",
    "    ((2012,7), (2013,6)), \n",
    "    ((2013,7), (2014,6)),  \n",
    "    ((2015,7), (2016,6)),  \n",
    "    ((2016,7), (2017,6)), \n",
    "    ((2016,7), (2017,6)), \n",
    "]\n",
    "\n",
    "models = {\n",
    "    'Ordinal_LogisticAT': mord.LogisticAT(alpha=1.0),  # 'alpha' controls regularization\n",
    "}\n",
    "\n",
    "def get_agg_lagged_features(factors):\n",
    "    return [f\"{f}_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_province_{t}\" for f in factors for t in range(3, 9)] + \\\n",
    "           [f\"{f}_country_{t}\" for f in factors for t in range(3, 9)]\n",
    "\n",
    "features = {\n",
    "    'traditional': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors\n",
    "    ),\n",
    "    \n",
    "    'news': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(news_factors)\n",
    "    ),\n",
    "    \n",
    "    'traditional+news': time_series.select(\n",
    "        ['year', 'month'] + \n",
    "        [f\"fews_ipc_{t}\" for t in range(3, 21, 3)] + \n",
    "        get_agg_lagged_features(t_variant_traditional_factors) + \n",
    "        t_invariant_traditional_factors + \n",
    "        get_agg_lagged_features(news_factors)\n",
    "    ),   \n",
    "}\n",
    "\n",
    "labels_df = time_series.select(['fews_ipc', 'year', 'month'])\n",
    "\n",
    "def get_time_split(df, start, end):\n",
    "    return df.filter(\n",
    "        ((pl.col('year') > start[0]) | ((pl.col('year') == start[0]) & (pl.col('month') >= start[1]))) &\n",
    "        ((pl.col('year') < end[0]) | ((pl.col('year') == end[0]) & (pl.col('month') <= end[1])))\n",
    "    )\n",
    "\n",
    "def train_and_evaluate(train, test, f, D):\n",
    "    results = []\n",
    "    \n",
    "    train_df = get_time_split(D, train[0], train[1])\n",
    "    X_train = train_df.drop(['year', 'month']).to_numpy()\n",
    "    \n",
    "    train_labels = get_time_split(labels_df, train[0], train[1])\n",
    "    y_train = train_labels.drop(['year', 'month']).to_numpy().ravel().astype(int)\n",
    "\n",
    "    test_df = get_time_split(D, test[0], test[1])\n",
    "    X_test = test_df.drop(['year', 'month']).to_numpy()\n",
    "    \n",
    "    test_labels = get_time_split(labels_df, test[0], test[1])\n",
    "    y_test = test_labels.drop(['year', 'month']).to_numpy().ravel().astype(int)\n",
    "    \n",
    "    train_mask = ~np.isnan(X_train).any(axis=1)\n",
    "    test_mask = ~np.isnan(X_test).any(axis=1)\n",
    "    \n",
    "    X_train = X_train[train_mask]\n",
    "    y_train = y_train[train_mask]\n",
    "    X_test = X_test[test_mask]\n",
    "    y_test = y_test[test_mask]\n",
    "\n",
    "    unique_classes = np.unique(y_train)\n",
    "\n",
    "    if X_train.shape[0] <= 0 or X_test.shape[0] <= 0:\n",
    "        return results\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        # Cross-Entropy loss using log_loss from sklearn\n",
    "        cross_entropy_loss = log_loss(y_test, model.predict_proba(X_test), labels=unique_classes)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "        results.append({\n",
    "            'method': name,\n",
    "            'split': test,\n",
    "            'features': f,\n",
    "            'cross_entropy_loss': cross_entropy_loss,\n",
    "            'accuracy': accuracy,\n",
    "        })\n",
    "        \n",
    "        print(f\"Method: {name}, Split: {test}, Features: {f}, Cross-Entropy Loss: {cross_entropy_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "all_results = []\n",
    "for train, test in zip(train_splits, test_splits):\n",
    "    for f, D in features.items():\n",
    "        try:\n",
    "            result = train_and_evaluate(train, test, f, D)\n",
    "            all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} on {train} & {test}\")\n",
    "            continue\n",
    "\n",
    "results_oc = pl.DataFrame([res for sublist in all_results for res in sublist])\n",
    "# results_oc.write_csv('fig_3a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>method</th><th>features</th><th>mean_cel</th><th>mean_acc</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Ordinal_LogisticAT&quot;</td><td>&quot;traditional+news&quot;</td><td>0.966862</td><td>0.6577</td></tr><tr><td>&quot;Ordinal_LogisticAT&quot;</td><td>&quot;news&quot;</td><td>0.021925</td><td>0.994161</td></tr><tr><td>&quot;Ordinal_LogisticAT&quot;</td><td>&quot;traditional&quot;</td><td>1.072231</td><td>0.656879</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ method             â”† features         â”† mean_cel â”† mean_acc â”‚\n",
       "â”‚ ---                â”† ---              â”† ---      â”† ---      â”‚\n",
       "â”‚ str                â”† str              â”† f64      â”† f64      â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ Ordinal_LogisticAT â”† traditional+news â”† 0.966862 â”† 0.6577   â”‚\n",
       "â”‚ Ordinal_LogisticAT â”† news             â”† 0.021925 â”† 0.994161 â”‚\n",
       "â”‚ Ordinal_LogisticAT â”† traditional      â”† 1.072231 â”† 0.656879 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_oc = results_oc.group_by(['method', 'features']).agg(\n",
    "    pl.mean('cross_entropy_loss').alias('mean_cel'),\n",
    "    pl.mean('accuracy').alias('mean_acc')   \n",
    ")\n",
    "\n",
    "agg_oc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
